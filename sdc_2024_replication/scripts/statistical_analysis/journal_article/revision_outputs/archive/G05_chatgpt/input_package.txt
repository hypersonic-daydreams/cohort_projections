# ChatGPT 5.2 Pro Session G5: Duration → Forecasting Bridge

## Context
The paper includes duration/hazard analysis of migration waves.
The goal is to connect this to operational forecasting, not just description.


## IMPORTANT: Data Context and Completeness

### What You Are Receiving
This package contains the COMPLETE analysis results (all JSON files) and key documentation.
However, some raw data files are filtered or summarized due to size constraints.

### Full Data Ecosystem (Not All Included)

| Dataset | Full Size | Included Here | Notes |
|---------|-----------|---------------|-------|
| nd_migration_summary.csv | 15 rows (2010-2024) | COMPLETE | Primary time series |
| combined_components_of_change.csv | 795 rows (53 states × 15 years) | COMPLETE | Full panel |
| refugee_arrivals_by_state_nationality.parquet | 15,447 rows | FILTERED | Only ND + MN + SD + MT |
| dhs_lpr_by_state_country.parquet | 8,619 rows | VARIES | May be filtered |
| acs_foreign_born_by_state_origin.parquet | 125,580 rows | NOT INCLUDED | Too large; use JSON summaries |
| acs_foreign_born_nd_share.parquet | 2,414 rows | NOT INCLUDED | Use JSON summaries |

### Key Temporal Notes
- **PEP data**: Calendar year (January-December)
- **Refugee/LPR data**: Fiscal year (October-September, so FY2020 = Oct 2019 - Sep 2020)
- **ACS data**: Multi-year estimates (e.g., 2019-2023 5-year) or 1-year estimates
- **Alignment**: FY data is typically aligned to the calendar year in which it ends

### Analysis Results Context
The JSON files contain COMPLETE analysis outputs from Python scripts that had access to
ALL raw data. You can trust these results as authoritative. The raw data included here
is for additional context and verification.

### What This Means for Your Analysis
1. Trust the JSON analysis results - they used complete data
2. When I note a dataset is FILTERED, I'll specify what filter was applied
3. If you need information about data not included, the JSON summaries contain key statistics
4. Your recommendations will be implemented by Claude Code, which has full data access


### Session-Specific Data Notes
- Duration analysis JSON results: COMPLETE - wave durations, hazard model estimates
- nd_migration_summary.csv: COMPLETE (15 rows, 2010-2024)
- Scenario modeling JSON: COMPLETE - for integration context
- Python script: COMPLETE - so you can verify the duration/hazard implementation

### Critical Note on Wave Definition
The duration analysis identifies 'migration waves' in the ND time series.
With only 15 years of data, there are very few complete waves observed.
The hazard model estimates are based on this limited sample of wave events.

Your task is to develop a THEORETICAL framework for connecting duration
analysis to forecasting. The implementation will be done by Claude Code
with access to all data. Focus on the conceptual and mathematical bridge.


================================================================================
FILE: CRITIQUE: ChatGPT_5-2-Pro_article_draft_critique.md
================================================================================

The following is a review and critique of 'sdc_2024_replication/scripts/statistical_analysis/journal_article/output/article_draft.pdf', provided by ChatGPT 5.2 Pro on 12/29/2025 at 2:15pm.

---

I read your draft **“Forecasting International Migration to North Dakota: A Multi-Method Empirical Analysis”** (Dec 29, 2025).

I’m going to respond in the voice of a friendly-but-picky senior demography reviewer for a top-tier outlet. Think: “revise & resubmit, with a real shot—if you tighten the target estimand, clean up identification, and make the forecasting contribution operational rather than panoramic.”

## High-level assessment

This is an unusually ambitious paper for a single-state migration forecasting problem. The **motivation is strong** (small-state volatility + policy sensitivity), the **data ecology is thoughtful** (PEP net international migration + DHS LPRs + ACS stocks + Refugee Processing Center arrivals), and your **core normative stance**—*stop pretending these forecasts are point-precise; quantify uncertainty and plan contingently*—is exactly right for practice and defensible academically.

Where the paper currently falls short of a top-journal bar is mostly *not* about effort or sophistication; it’s about **discipline**:

* **What is the estimand?** (What, precisely, are you forecasting?)
* **How do the modules connect to that estimand?**
* **Are the causal designs actually identified and correctly inferred?**
* **Do the forecasting claims survive backtesting and arithmetic?**

Right now, the paper sometimes reads like a very competent methods demo reel, when the journal will want: **one coherent forecasting framework** that is (a) theoretically grounded, (b) empirically validated, and (c) clearly implementable for projection work.

## Major issues to address before this is publishable

### 1) Clarify (and commit to) the forecasting target

You move among at least four “international migration” objects:

1. **PEP net international migration** (state component of change; includes multiple streams and is *net*, not gross inflow), used for the 2010–2024 time series and the panel.
2. **Refugee arrivals (RPC)** (initial placement; not net; FY basis), used for DiD and wave duration.
3. **LPR admissions (DHS)** (state of intended residence; one FY cross-section), used for gravity.
4. **Foreign-born stock (ACS)** (5-year estimates with MOE), used for diaspora and LQs.

That’s all fine *as inputs*, but a top-tier paper needs a clean statement like:

> “The forecast target for projection practice is **PEP net international migration to North Dakota (calendar year)**; the other sources are used to (i) decompose that net flow into interpretable substreams, (ii) estimate policy responsiveness, and (iii) build predictors.”

Right now, the reader can’t tell whether your headline claims (e.g., refugee dominance, network elasticities, policy effects) are meant to forecast **PEP net migration**, **refugee arrivals**, or **LPR inflows**—and those are not interchangeable.

**Concrete fix:** Add a short “Estimand & Measurement” subsection early in Methods:

* Define each measure formally (you already start doing this in the PEP section).
* Explain the mapping: e.g., show correlations/ratios between PEP net migration and RPC arrivals for overlapping years, and explicitly say what portion of PEP is plausibly “refugee-driven” vs “other” in ND.
* Align calendar vs fiscal year handling (right now FY vs calendar is used without consistently flagging when you translate).

Without that, the paper risks a reviewer’s classic verdict: *“Interesting analyses, but unclear what they estimate.”*

---

### 2) The “nine-module” approach needs a tighter narrative logic

You argue the breadth is for triangulation. That’s plausible, but top journals will ask: **Triangulation of what?** A single estimand? A set of mechanisms? A forecasting model class?

At minimum, each module should answer one of your four research questions *and* feed the forecasting framework. Some modules currently feel orphaned.

Examples:

* The **panel section** mostly decomposes state and year effects (Table 10), but doesn’t estimate substantive determinants (no covariates are shown, despite equation (8) including (x_{it})). That reads as descriptive, not explanatory—and not obviously forecasting-relevant.
* **Machine learning** is described (Elastic Net, Random Forest, K-means), but results are barely presented. Without out-of-sample evaluation, ML reads like name-dropping rather than contribution.
* **Duration/wave analysis** is cool, but it’s not yet connected to how ND will forecast future refugee waves.

**Concrete fix:** Decide whether the paper is:

* (A) a **forecasting paper** (then the core is predictive accuracy + calibration + uncertainty + implementability), or
* (B) a **migration dynamics + policy paper** (then forecasting is downstream and more qualitative).

Given your stated goal (improving projection practice under policy shocks), option (A) seems right. If so, you can still keep multi-method, but you must show a clear pipeline:

> “We forecast ND PEP net international migration by combining (i) a structural decomposition (refugee vs non-refugee), (ii) policy-sensitive submodels, and (iii) an ensemble calibrated via backtesting.”

Right now, the ensemble idea is gestured at, but not demonstrated.

---

### 3) Small-sample inference: you need to dial back “test-centric” language and use designs suited to (n=15)

You repeatedly acknowledge (n=15) annual observations (good), but you still lean on a lot of classical testing (Shapiro-Wilk, ADF, Chow, etc.) and interpret p-values with more confidence than the design warrants.

Two specific technical issues to fix:

**(i) KPSS interpretation contradiction.**
In the Results narrative you say KPSS “fails to reject stationarity in levels,” but your robustness table shows KPSS rejecting stationarity in levels (marked **). Those can’t both be true. (This is the sort of internal inconsistency that makes reviewers nervous about the rest.)

**(ii) Unit root vs structural break confusion.**
With a major break around 2020–2021, standard ADF tests are notorious for mistaking broken-trend stationarity for unit roots (and vice versa). Declaring the series a random walk without drift based on AIC selection in a 15-point annual series is not a safe inferential leap.

**Concrete fix:** Reframe time-series claims as *descriptive diagnostics* and add break-robust alternatives:

* Consider unit-root tests allowing a break (even if low power) and/or adopt a **state-space/local-level model** that naturally accommodates level shifts without pretending you learned “true I(1) structure” from 15 points.
* When you do report tests, write like: “consistent with” rather than “establishes.”

Also: call your forecast ranges **prediction intervals**, not “confidence intervals,” unless you’re very explicit about what is conditioned on what.

---

### 4) Forecasting contribution is currently under-validated

A top-tier forecasting paper lives or dies on: **out-of-sample performance, calibration, and comparison to benchmarks.**

Right now, you present:

* ARIMA(0,1,0) as “optimal” via AIC,
* scenario projections through 2045,
* Monte Carlo “credible intervals,”
* some model averaging weights.

But you do *not* show a proper **backtest**.

**Concrete fix:** Add a forecasting evaluation section that includes:

* Rolling-origin evaluation (e.g., train 2010–2016 predict 2017; train 2010–2017 predict 2018; etc.).
* Compare at least three baselines:

  * naïve last-observation (random walk),
  * mean/median benchmark,
  * a simple regression with a national driver (or refugee ceiling proxy, if you incorporate it).
* Show point accuracy metrics (MAE, RMSE) **and** interval calibration (coverage of 80/95% intervals).
* If you keep scenarios, be clear they are *policy-conditional narratives*, not “forecasts” in the statistical sense.

If you do this, your claim that “rigorous analysis remains feasible in small samples” becomes much more credible.

---

### 5) Gravity model: specification and interpretation problems

You do something important here—showing ND’s diaspora elasticity around ~0.10 (vs larger in gateway contexts)—but several issues need repair before a journal will accept the inference.

**(i) Distance and multilateral resistance**
In the “full gravity” spec you discuss distance conceptually, but the presented full specification appears to omit distance (while an earlier “simple” spec includes it). You should not imply that bilateral structure “implicitly controls for distance.” It doesn’t. If distance varies across destination states (it does), include it—or explain why you exclude it.

**(ii) Cross-section limits**
Using FY2023 only means you’re estimating a cross-sectional association, not a dynamic network effect. The paper sometimes slides into causal language (“causal network effect”) that you cannot defend without stronger identification (panel over time, plausibly exogenous stock variation, etc.).

**(iii) Standard errors look implausibly tiny**
SEs on diaspora elasticity reported at ~0.001–0.002 for PPML raise flags. That can happen with large samples, but with state-country data and many zeros, reviewers will immediately ask: robust variance? clustering? overdispersion? Any correction for the fact that diaspora stock is estimated (ACS) rather than measured?

**Concrete fix:**

* Make the “network effect” language more careful (association vs causal).
* Add either:

  * a true panel gravity (multiple years of admissions), or
  * a clearly defended IV strategy with correct inference, or
  * a forecasting-focused interpretation: “diaspora stock improves prediction modestly; causal interpretation is not claimed.”

Right now, you’re halfway between “prediction tool” and “causal mechanism,” and journals punish that limbo.

---

### 6) Causal inference: DiD and synthetic control need stricter identification and inference

This is the most policy-relevant part of the paper, so it must be the most bulletproof.

#### 6a) DiD on Travel Ban: inference and outcome model

You estimate a ~75% reduction for affected nationalities (log specification; strong result). The weak points:

* **Standard errors:** HC3 is not the default for DiD panel settings with repeated observations by nationality. Reviewers will expect **clustering at the nationality level** at minimum (and perhaps additional structure).
* **Outcome functional form:** (\ln(y+1)) with many zeros is common but can bias interpretation; count models (PPML with FE) are increasingly standard in migration flow contexts.
* **Parallel trends:** you report a pre-trend test and mention an event study figure, but the figure is missing (“Figure ??”). For a top-tier outlet, the event-study plot is not optional.

**Concrete fix:** Re-estimate DiD using PPML with country and year FE (and cluster properly), show the event study, and include sensitivity checks (alternative post periods; excluding 2017 as partial; placebo treated groups).

#### 6b) Synthetic control: fundamental design problem

A basic synthetic control requires untreated donor units. A national policy shock like the Travel Ban affects all states. So “synthetic ND” from other states is not a clean counterfactual unless you very explicitly define **treatment intensity** (e.g., ND is “high exposure” because of its pre-2017 composition, donors are “low exposure”). Even then, standard SCM needs adaptation.

Right now, the SCM is presented as if it creates an untreated counterfactual, which a reviewer will reject.

**Concrete fix:** Either:

* drop SCM, or
* reframe it as an *exposure-weighted* design (generalized synthetic control / interactive fixed effects / augmented SCM), where the identifying variation is differential exposure, not treated vs untreated.

#### 6c) Bartik shift-share: needs careful modern inference

Shift-share instruments are powerful but currently under intense methodological scrutiny. Journals will ask:

* What is the base period (t_0)?
* Are shares truly predetermined?
* Are standard errors computed using appropriate procedures for shift-share (not just vanilla robust SE)?
* What exactly is the unit and dependent variable in the IV model? (Your Table 7 coefficient “4.36” is hard to interpret without units.)

**Concrete fix:** Spell out the shift-share construction and use a defensible inference approach; otherwise, this will not survive review.

---

### 7) Scenario projections: there are arithmetic and definitional inconsistencies that must be fixed

Table 9 is where practice meets paper. It must be impeccable. Right now it isn’t.

* “**8% annual growth**” does not appear to compound to the reported 2045 value from the stated 2024 baseline (5,126). If it’s not compounding, say so; if it is, the numbers need to match.
* “**Continue 2010–2019 slope (+72/year)**” yields a **lower** 2045 projection (2,517) than the 2024 baseline (5,126). That’s logically inconsistent unless you are anchoring the projection to a pre-2020 level rather than the 2024 baseline—yet the note says baseline is 2024.
* The paper emphasizes **CV = 82.5%** in descriptive stats, but the Monte Carlo scenario uses **CV = 0.39**. That discrepancy needs explanation.

Also: you call the Monte Carlo intervals “credible intervals,” but the described procedure is a **frequentist simulation/parametric bootstrap** unless you’ve actually specified priors and a posterior. Journals care about that terminology.

**Concrete fix:** Add an appendix that shows the exact equations used for each scenario and the Monte Carlo process (what distribution, what parameters, what’s held fixed, what’s sampled). Then ensure the table numbers can be reproduced from those equations.

---

## Writing and presentation issues that will block publication if not fixed

### 8) Missing references and missing figures

The draft contains many placeholder citations “(?)” and “(??)” and figure references “Figure ??” (e.g., ACF/PACF; event study). In a top-tier journal, this is a hard stop: reviewers can’t assess novelty or correctness without the literature anchoring and the empirical visuals.

**Concrete fix:** Add a full references section, replace every placeholder, and include every cited figure/table.

### 9) Tone: occasionally too self-congratulatory / declarative

Phrases like “honestly characterizing” and repeated “this demonstrates that…” can read as defensive or overconfident. In top-journal style, you can keep the clarity but shift toward restrained claims:

* “consistent with,” “suggests,” “within this short series,” “in this setting,” etc.

This isn’t about humility theater; it’s about aligning claims with identification strength.

### 10) ACS measurement error and MOE not fully integrated

You correctly note ACS uncertainty, but tables report point estimates without MOE and then treat derived quantities (LQs, diaspora stock) as fixed regressors. Reviewers in demography are sensitized to this.

**Concrete fix:** At minimum:

* note which ACS file (1-year vs 5-year) and show MOE for key LQ entries,
* do sensitivity analysis aggregating origins where MOE is high,
* consider measurement-error attenuation in gravity models.

### 11) Calendar year vs fiscal year harmonization

You mix calendar-year PEP, FY refugee arrivals, FY LPR admissions, and multi-year ACS. That’s inevitable, but you need an explicit harmonization strategy and consistent language (e.g., “FY arrivals” vs “calendar-year net migration”).

---

## A strong “revise & resubmit” path

If you want this to land as a serious top-tier contribution *and* remain useful for ND projection practice, here’s the cleanest revision strategy:

1. **Define one forecast target** (PEP net international migration to ND) and treat everything else as model inputs.
2. **Decompose** that target into at least two latent/observable components:

   * refugee-related (policy-driven),
   * non-refugee international migration (economic/network-driven).
3. **Build a small-sample-friendly forecasting model** (state-space / Bayesian hierarchical / partial pooling) that can ingest national policy indicators as covariates or regimes.
4. **Backtest it** and compare to naïve benchmarks.
5. Keep scenarios, but treat them as **policy-conditional narratives** layered on top of the statistical model, with transparent arithmetic.

This keeps your core insight intact: small-state migration forecasting is not about pretending to know; it’s about structuring uncertainty and identifying which levers (policy, networks, resettlement capacity) move the distribution.

## Minor comments (still worth fixing)

* Several tables have duplicated “Notes:” lines (cosmetic, but signals draftiness).
* Table labeling between the main text and appendix appears inconsistent for the gravity model (Table 6 vs Table 11).
* Consider reporting per-capita rates more consistently (you do in Figure 1’s SCM discussion; make it systematic).
* Some statistical reporting is overly precise given the data (e.g., very small p-values with tiny n); rounding and emphasizing effect sizes will read better.

---

## Bottom line

This is a **promising and unusually relevant** piece—especially for the under-studied “peripheral state” context. But to meet a top-tier standard you need to (1) tighten the estimand, (2) repair identification/inference in the causal sections, (3) validate the forecasting claims with backtesting, and (4) fix internal inconsistencies (KPSS narrative, scenario arithmetic, missing figures/references).

Do that, and you’ll have something that can legitimately claim both scholarly value and operational usefulness—rare air in migration forecasting.


================================================================================
FILE: RESULTS: module_8_duration_analysis.json
================================================================================

{
  "module": "8",
  "analysis": "duration_analysis_survival",
  "generated": "2025-12-29T06:19:45.794597+00:00",
  "input_files": [
    "refugee_arrivals_by_state_nationality.parquet",
    "acs_foreign_born_by_state_origin.parquet"
  ],
  "parameters": {
    "wave_definition": {
      "threshold_percent_above_baseline": 50.0,
      "minimum_consecutive_years": 2,
      "baseline_calculation": "median of first half of observation period"
    },
    "survival_analysis": {
      "method": "Kaplan-Meier",
      "censoring": "right censoring for ongoing waves at end of data period",
      "cox_covariates": [
        "log_intensity",
        "high_intensity",
        "early_wave",
        "peak_arrivals",
        "nationality_region",
        "state_region"
      ]
    },
    "data_period": {
      "start_year": 2002,
      "end_year": 2020
    }
  },
  "results": {
    "wave_identification": {
      "total_waves_identified": 940,
      "unique_nationalities": 56,
      "unique_states": 48,
      "mean_wave_duration": 3.5393617021276595,
      "max_wave_duration": 13,
      "censored_waves": 94
    },
    "kaplan_meier": {
      "overall_summary": {
        "n_subjects": 940,
        "n_events": 846,
        "n_censored": 94,
        "censoring_rate": 10.0,
        "median_survival_years": 3.0,
        "median_survival_95ci": null,
        "mean_duration": 3.5393617021276595,
        "max_duration": 13,
        "min_duration": 2
      },
      "life_table": [
        {
          "time_years": 1,
          "n_at_risk": 940,
          "n_events": 0,
          "n_censored": 0,
          "survival_probability": 1.0,
          "ci_95_lower": null,
          "ci_95_upper": null
        },
        {
          "time_years": 2,
          "n_at_risk": 940,
          "n_events": 455,
          "n_censored": 14,
          "survival_probability": 0.5159574468085106,
          "ci_95_lower": 0.4835315851480659,
          "ci_95_upper": 0.5473749398559751
        },
        {
          "time_years": 3,
          "n_at_risk": 471,
          "n_events": 181,
          "n_censored": 5,
          "survival_probability": 0.3176808058905904,
          "ci_95_lower": 0.2878899020957237,
          "ci_95_upper": 0.3478353254438474
        },
        {
          "time_years": 4,
          "n_at_risk": 285,
          "n_events": 91,
          "n_censored": 3,
          "survival_probability": 0.21624588190447203,
          "ci_95_lower": 0.1901518823388554,
          "ci_95_upper": 0.24348300049728872
        },
        {
          "time_years": 5,
          "n_at_risk": 191,
          "n_events": 35,
          "n_censored": 9,
          "survival_probability": 0.17661967317852165,
          "ci_95_lower": 0.1525626844506243,
          "ci_95_upper": 0.20215177642870424
        },
        {
          "time_years": 6,
          "n_at_risk": 147,
          "n_events": 29,
          "n_censored": 10,
          "survival_probability": 0.14177633629296305,
          "ci_95_lower": 0.1197676145071941,
          "ci_95_upper": 0.1655938459866632
        },
        {
          "time_years": 7,
          "n_at_risk": 108,
          "n_events": 7,
          "n_censored": 10,
          "survival_probability": 0.13258712931101174,
          "ci_95_lower": 0.11112939704651743,
          "ci_95_upper": 0.1559598346885874
        },
        {
          "time_years": 8,
          "n_at_risk": 91,
          "n_events": 10,
          "n_censored": 13,
          "survival_probability": 0.11801711510101053,
          "ci_95_lower": 0.09738465345213887,
          "ci_95_upper": 0.14077128140824685
        },
        {
          "time_years": 9,
          "n_at_risk": 68,
          "n_events": 8,
          "n_censored": 11,
          "survival_probability": 0.10413274861853865,
          "ci_95_lower": 0.08416593749327285,
          "ci_95_upper": 0.12649880520579335
        },
        {
          "time_years": 10,
          "n_at_risk": 49,
          "n_events": 20,
          "n_censored": 3,
          "survival_probability": 0.06162958591709432,
          "ci_95_lower": 0.04445160351572917,
          "ci_95_upper": 0.08256552052629569
        },
        {
          "time_years": 11,
          "n_at_risk": 26,
          "n_events": 9,
          "n_censored": 7,
          "survival_probability": 0.040296267715023204,
          "ci_95_lower": 0.025822963576129315,
          "ci_95_upper": 0.05956676153578599
        },
        {
          "time_years": 12,
          "n_at_risk": 10,
          "n_events": 1,
          "n_censored": 8,
          "survival_probability": 0.03626664094352088,
          "ci_95_lower": 0.022011346644022847,
          "ci_95_upper": 0.055974773697811925
        },
        {
          "time_years": 13,
          "n_at_risk": 1,
          "n_events": 0,
          "n_censored": 1,
          "survival_probability": 0.03626664094352088,
          "ci_95_lower": 0.022011346644022847,
          "ci_95_upper": 0.055974773697811925
        }
      ],
      "method": "Kaplan-Meier product-limit estimator",
      "confidence_level": 0.95
    },
    "kaplan_meier_by_region": {
      "group_variable": "nationality_region",
      "groups": {
        "Middle East": {
          "n_subjects": 174,
          "n_events": 146,
          "median_survival": 3.0,
          "mean_duration": 4.137931034482759
        },
        "Asia": {
          "n_subjects": 67,
          "n_events": 67,
          "median_survival": 3.0,
          "mean_duration": 4.2835820895522385
        },
        "Africa": {
          "n_subjects": 353,
          "n_events": 312,
          "median_survival": 3.0,
          "mean_duration": 3.991501416430595
        },
        "Europe": {
          "n_subjects": 55,
          "n_events": 52,
          "median_survival": 2.0,
          "mean_duration": 2.4
        },
        "Other": {
          "n_subjects": 259,
          "n_events": 237,
          "median_survival": 2.0,
          "mean_duration": 2.72972972972973
        },
        "Americas": {
          "n_subjects": 32,
          "n_events": 32,
          "median_survival": 2.0,
          "mean_duration": 2.25
        }
      },
      "log_rank_test": {
        "test_statistic": 89.67706532074526,
        "p_value": 7.855240900207938e-18,
        "degrees_of_freedom": 5,
        "significant_at_05": "True"
      },
      "km_fitters": {
        "Middle East": "<lifelines.KaplanMeierFitter:\"Middle East\", fitted with 174 total observations, 28 right-censored observations>",
        "Asia": "<lifelines.KaplanMeierFitter:\"Asia\", fitted with 67 total observations, 0 right-censored observations>",
        "Africa": "<lifelines.KaplanMeierFitter:\"Africa\", fitted with 353 total observations, 41 right-censored observations>",
        "Europe": "<lifelines.KaplanMeierFitter:\"Europe\", fitted with 55 total observations, 3 right-censored observations>",
        "Other": "<lifelines.KaplanMeierFitter:\"Other\", fitted with 259 total observations, 22 right-censored observations>",
        "Americas": "<lifelines.KaplanMeierFitter:\"Americas\", fitted with 32 total observations, 0 right-censored observations>"
      }
    },
    "kaplan_meier_by_intensity": {
      "group_variable": "intensity_quartile",
      "groups": {
        "Q1 (Low)": {
          "n_subjects": 247,
          "n_events": 246,
          "median_survival": 2.0,
          "mean_duration": 2.42914979757085
        },
        "Q3": {
          "n_subjects": 236,
          "n_events": 223,
          "median_survival": 3.0,
          "mean_duration": 3.6059322033898304
        },
        "Q4 (High)": {
          "n_subjects": 234,
          "n_events": 160,
          "median_survival": 4.0,
          "mean_duration": 5.35042735042735
        },
        "Q2": {
          "n_subjects": 223,
          "n_events": 217,
          "median_survival": 2.0,
          "mean_duration": 2.798206278026906
        }
      },
      "log_rank_test": {
        "test_statistic": 278.7443670491838,
        "p_value": 3.9584267764539434e-60,
        "degrees_of_freedom": 3,
        "significant_at_05": "True"
      },
      "km_fitters": {
        "Q1 (Low)": "<lifelines.KaplanMeierFitter:\"Q1 (Low)\", fitted with 247 total observations, 1 right-censored observations>",
        "Q3": "<lifelines.KaplanMeierFitter:\"Q3\", fitted with 236 total observations, 13 right-censored observations>",
        "Q4 (High)": "<lifelines.KaplanMeierFitter:\"Q4 (High)\", fitted with 234 total observations, 74 right-censored observations>",
        "Q2": "<lifelines.KaplanMeierFitter:\"Q2\", fitted with 223 total observations, 6 right-censored observations>"
      }
    },
    "kaplan_meier_by_timing": {
      "group_variable": "early_wave",
      "groups": {
        "0": {
          "n_subjects": 433,
          "n_events": 359,
          "median_survival": 3.0,
          "mean_duration": 3.5450346420323324
        },
        "1": {
          "n_subjects": 507,
          "n_events": 487,
          "median_survival": 2.0,
          "mean_duration": 3.534516765285996
        }
      },
      "log_rank_test": {
        "test_statistic": 10.486225938263882,
        "p_value": 0.0012026778915378675,
        "degrees_of_freedom": 1,
        "significant_at_05": "True"
      }
    },
    "cox_proportional_hazards": {
      "model_type": "Cox Proportional Hazards",
      "coefficient_table": {
        "log_intensity": {
          "coefficient": -0.8869125161085291,
          "hazard_ratio": 0.4119256050894812,
          "std_error": 0.09488463855318664,
          "z_statistic": -9.34727190441242,
          "p_value": 8.99375132135104e-21,
          "hr_ci_95_lower": 0.3420210512816074,
          "hr_ci_95_upper": 0.4961177199254462
        },
        "high_intensity": {
          "coefficient": 0.024929142620664244,
          "hazard_ratio": 1.025242471955869,
          "std_error": 0.12767420218957684,
          "z_statistic": 0.1952559106940668,
          "p_value": 0.8451926108679167,
          "hr_ci_95_lower": 0.7982705567679182,
          "hr_ci_95_upper": 1.316749211643258
        },
        "early_wave": {
          "coefficient": 0.3078572503129925,
          "hazard_ratio": 1.3605067631600327,
          "std_error": 0.07386103844605026,
          "z_statistic": 4.168060140907148,
          "p_value": 3.0720288999311804e-05,
          "hr_ci_95_lower": 1.1771453058755719,
          "hr_ci_95_upper": 1.5724300503644397
        },
        "peak_arrivals": {
          "coefficient": -0.4219068931449047,
          "hazard_ratio": 0.6557950955699587,
          "std_error": 0.11797498209892578,
          "z_statistic": -3.576240365869455,
          "p_value": 0.0003485711736633715,
          "hr_ci_95_lower": 0.5204124497864817,
          "hr_ci_95_upper": 0.8263968464821743
        },
        "nationality_region_Americas": {
          "coefficient": 0.5368546615931535,
          "hazard_ratio": 1.7106179193620354,
          "std_error": 0.19128321610600918,
          "z_statistic": 2.8065957511694526,
          "p_value": 0.005006802990364509,
          "hr_ci_95_lower": 1.1757972723733479,
          "hr_ci_95_upper": 2.4887059485483705
        },
        "nationality_region_Asia": {
          "coefficient": -0.2799717451011243,
          "hazard_ratio": 0.755805096350601,
          "std_error": 0.14913311995535888,
          "z_statistic": -1.877327753787558,
          "p_value": 0.06047318661403387,
          "hr_ci_95_lower": 0.5642446814997957,
          "hr_ci_95_upper": 1.0124000498350256
        },
        "nationality_region_Europe": {
          "coefficient": 0.4500500006307893,
          "hazard_ratio": 1.5683906040491902,
          "std_error": 0.1550881540146248,
          "z_statistic": 2.901897978541608,
          "p_value": 0.0037090931272734506,
          "hr_ci_95_lower": 1.157292012876973,
          "hr_ci_95_upper": 2.1255215274101094
        },
        "nationality_region_Middle East": {
          "coefficient": 0.1375345493554996,
          "hazard_ratio": 1.147441348646323,
          "std_error": 0.10189014373254114,
          "z_statistic": 1.3498317336417158,
          "p_value": 0.1770699632422349,
          "hr_ci_95_lower": 0.9397264443318588,
          "hr_ci_95_upper": 1.401069062730702
        },
        "nationality_region_Other": {
          "coefficient": 0.5087493541010575,
          "hazard_ratio": 1.6632098071532297,
          "std_error": 0.09363436794824198,
          "z_statistic": 5.433361331410679,
          "p_value": 5.530226313982551e-08,
          "hr_ci_95_lower": 1.3843481308654215,
          "hr_ci_95_upper": 1.9982450952430293
        },
        "state_region_Northeast": {
          "coefficient": -0.09715111258818476,
          "hazard_ratio": 0.9074188733577685,
          "std_error": 0.14031805367868636,
          "z_statistic": -0.6923635985619544,
          "p_value": 0.48870901970377767,
          "hr_ci_95_lower": 0.6892374453560215,
          "hr_ci_95_upper": 1.1946666816695586
        },
        "state_region_Other": {
          "coefficient": 0.08196413614408432,
          "hazard_ratio": 1.0854168818968395,
          "std_error": 0.10534146291747255,
          "z_statistic": 0.7780804810760727,
          "p_value": 0.43652156886562377,
          "hr_ci_95_lower": 0.8829370806554062,
          "hr_ci_95_upper": 1.334330422086395
        },
        "state_region_South": {
          "coefficient": 0.024903261996922825,
          "hazard_ratio": 1.025215938384563,
          "std_error": 0.11610552544370338,
          "z_statistic": 0.21448817273556703,
          "p_value": 0.8301663891500295,
          "hr_ci_95_lower": 0.8165563243516698,
          "hr_ci_95_upper": 1.2871956152593242
        },
        "state_region_West": {
          "coefficient": -0.10321984419595252,
          "hazard_ratio": 0.9019286679013484,
          "std_error": 0.12940341601028574,
          "z_statistic": -0.7976593460851761,
          "p_value": 0.42506820175115345,
          "hr_ci_95_lower": 0.6998803556463181,
          "hr_ci_95_upper": 1.1623062648059055
        }
      },
      "fit_statistics": {
        "log_likelihood": -4877.809233034444,
        "aic": 9781.618466068889,
        "bic": 9844.61490444732,
        "concordance_index": 0.7692550588546488,
        "concordance_se": null,
        "n_observations": 940,
        "n_events": 846
      },
      "proportional_hazards_test": {
        "test": "Schoenfeld residuals",
        "results_by_variable": {
          "log_intensity": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "high_intensity": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "early_wave": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "peak_arrivals": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "nationality_region_Americas": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "nationality_region_Asia": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "nationality_region_Europe": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "nationality_region_Middle East": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "nationality_region_Other": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "state_region_Northeast": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "state_region_Other": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "state_region_South": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          },
          "state_region_West": {
            "test_statistic": null,
            "p_value": null,
            "assumption_violated": false
          }
        },
        "global_test_passed": true
      },
      "interpretation": {
        "concordance": "Good",
        "significant_predictors": [
          "log_intensity",
          "early_wave",
          "peak_arrivals",
          "nationality_region_Americas",
          "nationality_region_Europe",
          "nationality_region_Other"
        ]
      }
    },
    "lifecycle_analysis": {
      "aggregate_statistics": {
        "mean_time_to_peak": 2.1893617021276595,
        "median_time_to_peak": 2.0,
        "mean_initiation_share": 0.28145599081769296,
        "mean_decline_share": 0.3490800186810825,
        "mean_total_duration": 3.5393617021276595
      },
      "by_nationality": {
        "Iraq": {
          "mean_duration": 5.280701754385965,
          "mean_time_to_peak": 2.8421052631578947,
          "total_peak_arrivals": 26931.0
        },
        "Burma": {
          "mean_duration": 5.525,
          "mean_time_to_peak": 3.25,
          "total_peak_arrivals": 23737.0
        },
        "Somalia": {
          "mean_duration": 3.2903225806451615,
          "mean_time_to_peak": 2.2096774193548385,
          "total_peak_arrivals": 22004.0
        },
        "Democratic Republic of the Congo": {
          "mean_duration": 6.483333333333333,
          "mean_time_to_peak": 4.35,
          "total_peak_arrivals": 18533.0
        },
        "USSR": {
          "mean_duration": 2.375,
          "mean_time_to_peak": 1.5,
          "total_peak_arrivals": 14172.0
        },
        "Syria": {
          "mean_duration": 4.324324324324325,
          "mean_time_to_peak": 2.27027027027027,
          "total_peak_arrivals": 12365.0
        },
        "Laos": {
          "mean_duration": 2.6,
          "mean_time_to_peak": 1.0,
          "total_peak_arrivals": 8672.0
        },
        "Liberia": {
          "mean_duration": 3.085714285714286,
          "mean_time_to_peak": 1.8571428571428572,
          "total_peak_arrivals": 6529.0
        },
        "Iran": {
          "mean_duration": 2.806451612903226,
          "mean_time_to_peak": 1.967741935483871,
          "total_peak_arrivals": 6234.0
        },
        "Burundi": {
          "mean_duration": 2.238095238095238,
          "mean_time_to_peak": 1.2619047619047619,
          "total_peak_arrivals": 5410.0
        }
      },
      "n_waves_analyzed": 940
    }
  },
  "diagnostics": {
    "sample_sizes": {
      "total_waves": 940,
      "survival_analysis_n": 940,
      "cox_model_n": 940
    },
    "censoring": {
      "rate": 10.0,
      "reason": "Wave ongoing at end of data period (FY2020)"
    },
    "model_fit": {
      "concordance_index": 0.7692550588546488,
      "aic": 9781.618466068889,
      "bic": 9844.61490444732
    }
  },
  "warnings": [],
  "decisions": [
    {
      "decision_id": "D001",
      "timestamp": "2025-12-29T06:19:43.964352+00:00",
      "category": "wave_definition",
      "decision": "Defined immigration wave as 50.0% above baseline for 2+ years",
      "rationale": "50% threshold captures significant departures from normal; 2-year minimum filters noise",
      "alternatives_considered": [
        "100% above baseline",
        "1 standard deviation above mean",
        "Moving average crossing"
      ],
      "evidence": "Based on visual inspection of refugee arrival patterns",
      "reversible": true
    },
    {
      "decision_id": "D002",
      "timestamp": "2025-12-29T06:19:43.686277+00:00",
      "category": "methodology",
      "decision": "Used Cox Proportional Hazards model for duration analysis",
      "rationale": "Cox PH allows multiple covariates and handles censored observations",
      "alternatives_considered": [
        "Accelerated failure time models",
        "Parametric survival models"
      ],
      "evidence": "Concordance index: 0.7693",
      "reversible": true
    }
  ],
  "next_steps": [
    "Compare wave patterns to overall immigration trends (Module 1)",
    "Integrate lifecycle phases with time series forecasts (Module 2)",
    "Use hazard ratios to inform scenario projections (Module 5)",
    "Validate wave definitions with domain experts"
  ]
}


================================================================================
FILE: RESULTS: module_8_hazard_model.json
================================================================================

{
  "model": {
    "model_type": "Cox Proportional Hazards",
    "coefficient_table": {
      "log_intensity": {
        "coefficient": -0.8869125161085291,
        "hazard_ratio": 0.4119256050894812,
        "std_error": 0.09488463855318664,
        "z_statistic": -9.34727190441242,
        "p_value": 8.99375132135104e-21,
        "hr_ci_95_lower": 0.3420210512816074,
        "hr_ci_95_upper": 0.4961177199254462
      },
      "high_intensity": {
        "coefficient": 0.024929142620664244,
        "hazard_ratio": 1.025242471955869,
        "std_error": 0.12767420218957684,
        "z_statistic": 0.1952559106940668,
        "p_value": 0.8451926108679167,
        "hr_ci_95_lower": 0.7982705567679182,
        "hr_ci_95_upper": 1.316749211643258
      },
      "early_wave": {
        "coefficient": 0.3078572503129925,
        "hazard_ratio": 1.3605067631600327,
        "std_error": 0.07386103844605026,
        "z_statistic": 4.168060140907148,
        "p_value": 3.0720288999311804e-05,
        "hr_ci_95_lower": 1.1771453058755719,
        "hr_ci_95_upper": 1.5724300503644397
      },
      "peak_arrivals": {
        "coefficient": -0.4219068931449047,
        "hazard_ratio": 0.6557950955699587,
        "std_error": 0.11797498209892578,
        "z_statistic": -3.576240365869455,
        "p_value": 0.0003485711736633715,
        "hr_ci_95_lower": 0.5204124497864817,
        "hr_ci_95_upper": 0.8263968464821743
      },
      "nationality_region_Americas": {
        "coefficient": 0.5368546615931535,
        "hazard_ratio": 1.7106179193620354,
        "std_error": 0.19128321610600918,
        "z_statistic": 2.8065957511694526,
        "p_value": 0.005006802990364509,
        "hr_ci_95_lower": 1.1757972723733479,
        "hr_ci_95_upper": 2.4887059485483705
      },
      "nationality_region_Asia": {
        "coefficient": -0.2799717451011243,
        "hazard_ratio": 0.755805096350601,
        "std_error": 0.14913311995535888,
        "z_statistic": -1.877327753787558,
        "p_value": 0.06047318661403387,
        "hr_ci_95_lower": 0.5642446814997957,
        "hr_ci_95_upper": 1.0124000498350256
      },
      "nationality_region_Europe": {
        "coefficient": 0.4500500006307893,
        "hazard_ratio": 1.5683906040491902,
        "std_error": 0.1550881540146248,
        "z_statistic": 2.901897978541608,
        "p_value": 0.0037090931272734506,
        "hr_ci_95_lower": 1.157292012876973,
        "hr_ci_95_upper": 2.1255215274101094
      },
      "nationality_region_Middle East": {
        "coefficient": 0.1375345493554996,
        "hazard_ratio": 1.147441348646323,
        "std_error": 0.10189014373254114,
        "z_statistic": 1.3498317336417158,
        "p_value": 0.1770699632422349,
        "hr_ci_95_lower": 0.9397264443318588,
        "hr_ci_95_upper": 1.401069062730702
      },
      "nationality_region_Other": {
        "coefficient": 0.5087493541010575,
        "hazard_ratio": 1.6632098071532297,
        "std_error": 0.09363436794824198,
        "z_statistic": 5.433361331410679,
        "p_value": 5.530226313982551e-08,
        "hr_ci_95_lower": 1.3843481308654215,
        "hr_ci_95_upper": 1.9982450952430293
      },
      "state_region_Northeast": {
        "coefficient": -0.09715111258818476,
        "hazard_ratio": 0.9074188733577685,
        "std_error": 0.14031805367868636,
        "z_statistic": -0.6923635985619544,
        "p_value": 0.48870901970377767,
        "hr_ci_95_lower": 0.6892374453560215,
        "hr_ci_95_upper": 1.1946666816695586
      },
      "state_region_Other": {
        "coefficient": 0.08196413614408432,
        "hazard_ratio": 1.0854168818968395,
        "std_error": 0.10534146291747255,
        "z_statistic": 0.7780804810760727,
        "p_value": 0.43652156886562377,
        "hr_ci_95_lower": 0.8829370806554062,
        "hr_ci_95_upper": 1.334330422086395
      },
      "state_region_South": {
        "coefficient": 0.024903261996922825,
        "hazard_ratio": 1.025215938384563,
        "std_error": 0.11610552544370338,
        "z_statistic": 0.21448817273556703,
        "p_value": 0.8301663891500295,
        "hr_ci_95_lower": 0.8165563243516698,
        "hr_ci_95_upper": 1.2871956152593242
      },
      "state_region_West": {
        "coefficient": -0.10321984419595252,
        "hazard_ratio": 0.9019286679013484,
        "std_error": 0.12940341601028574,
        "z_statistic": -0.7976593460851761,
        "p_value": 0.42506820175115345,
        "hr_ci_95_lower": 0.6998803556463181,
        "hr_ci_95_upper": 1.1623062648059055
      }
    },
    "fit_statistics": {
      "log_likelihood": -4877.809233034444,
      "aic": 9781.618466068889,
      "bic": 9844.61490444732,
      "concordance_index": 0.7692550588546488,
      "concordance_se": null,
      "n_observations": 940,
      "n_events": 846
    },
    "proportional_hazards_test": {
      "test": "Schoenfeld residuals",
      "results_by_variable": {
        "log_intensity": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "high_intensity": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "early_wave": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "peak_arrivals": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "nationality_region_Americas": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "nationality_region_Asia": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "nationality_region_Europe": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "nationality_region_Middle East": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "nationality_region_Other": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "state_region_Northeast": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "state_region_Other": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "state_region_South": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        },
        "state_region_West": {
          "test_statistic": null,
          "p_value": null,
          "assumption_violated": false
        }
      },
      "global_test_passed": true
    },
    "interpretation": {
      "concordance": "Good",
      "significant_predictors": [
        "log_intensity",
        "early_wave",
        "peak_arrivals",
        "nationality_region_Americas",
        "nationality_region_Europe",
        "nationality_region_Other"
      ]
    }
  },
  "lifecycle_analysis": {
    "aggregate_statistics": {
      "mean_time_to_peak": 2.1893617021276595,
      "median_time_to_peak": 2.0,
      "mean_initiation_share": 0.28145599081769296,
      "mean_decline_share": 0.3490800186810825,
      "mean_total_duration": 3.5393617021276595
    },
    "by_nationality": {
      "Iraq": {
        "mean_duration": 5.280701754385965,
        "mean_time_to_peak": 2.8421052631578947,
        "total_peak_arrivals": 26931.0
      },
      "Burma": {
        "mean_duration": 5.525,
        "mean_time_to_peak": 3.25,
        "total_peak_arrivals": 23737.0
      },
      "Somalia": {
        "mean_duration": 3.2903225806451615,
        "mean_time_to_peak": 2.2096774193548385,
        "total_peak_arrivals": 22004.0
      },
      "Democratic Republic of the Congo": {
        "mean_duration": 6.483333333333333,
        "mean_time_to_peak": 4.35,
        "total_peak_arrivals": 18533.0
      },
      "USSR": {
        "mean_duration": 2.375,
        "mean_time_to_peak": 1.5,
        "total_peak_arrivals": 14172.0
      },
      "Syria": {
        "mean_duration": 4.324324324324325,
        "mean_time_to_peak": 2.27027027027027,
        "total_peak_arrivals": 12365.0
      },
      "Laos": {
        "mean_duration": 2.6,
        "mean_time_to_peak": 1.0,
        "total_peak_arrivals": 8672.0
      },
      "Liberia": {
        "mean_duration": 3.085714285714286,
        "mean_time_to_peak": 1.8571428571428572,
        "total_peak_arrivals": 6529.0
      },
      "Iran": {
        "mean_duration": 2.806451612903226,
        "mean_time_to_peak": 1.967741935483871,
        "total_peak_arrivals": 6234.0
      },
      "Burundi": {
        "mean_duration": 2.238095238095238,
        "mean_time_to_peak": 1.2619047619047619,
        "total_peak_arrivals": 5410.0
      }
    },
    "n_waves_analyzed": 940
  }
}


================================================================================
FILE: RESULTS: module_8_wave_durations.json
================================================================================

{
  "summary": {
    "total_waves_identified": 940,
    "unique_nationalities": 56,
    "unique_states": 48,
    "mean_wave_duration": 3.5393617021276595,
    "max_wave_duration": 13,
    "censored_waves": 94
  },
  "kaplan_meier": {
    "overall_summary": {
      "n_subjects": 940,
      "n_events": 846,
      "n_censored": 94,
      "censoring_rate": 10.0,
      "median_survival_years": 3.0,
      "median_survival_95ci": null,
      "mean_duration": 3.5393617021276595,
      "max_duration": 13,
      "min_duration": 2
    },
    "life_table": [
      {
        "time_years": 1,
        "n_at_risk": 940,
        "n_events": 0,
        "n_censored": 0,
        "survival_probability": 1.0,
        "ci_95_lower": null,
        "ci_95_upper": null
      },
      {
        "time_years": 2,
        "n_at_risk": 940,
        "n_events": 455,
        "n_censored": 14,
        "survival_probability": 0.5159574468085106,
        "ci_95_lower": 0.4835315851480659,
        "ci_95_upper": 0.5473749398559751
      },
      {
        "time_years": 3,
        "n_at_risk": 471,
        "n_events": 181,
        "n_censored": 5,
        "survival_probability": 0.3176808058905904,
        "ci_95_lower": 0.2878899020957237,
        "ci_95_upper": 0.3478353254438474
      },
      {
        "time_years": 4,
        "n_at_risk": 285,
        "n_events": 91,
        "n_censored": 3,
        "survival_probability": 0.21624588190447203,
        "ci_95_lower": 0.1901518823388554,
        "ci_95_upper": 0.24348300049728872
      },
      {
        "time_years": 5,
        "n_at_risk": 191,
        "n_events": 35,
        "n_censored": 9,
        "survival_probability": 0.17661967317852165,
        "ci_95_lower": 0.1525626844506243,
        "ci_95_upper": 0.20215177642870424
      },
      {
        "time_years": 6,
        "n_at_risk": 147,
        "n_events": 29,
        "n_censored": 10,
        "survival_probability": 0.14177633629296305,
        "ci_95_lower": 0.1197676145071941,
        "ci_95_upper": 0.1655938459866632
      },
      {
        "time_years": 7,
        "n_at_risk": 108,
        "n_events": 7,
        "n_censored": 10,
        "survival_probability": 0.13258712931101174,
        "ci_95_lower": 0.11112939704651743,
        "ci_95_upper": 0.1559598346885874
      },
      {
        "time_years": 8,
        "n_at_risk": 91,
        "n_events": 10,
        "n_censored": 13,
        "survival_probability": 0.11801711510101053,
        "ci_95_lower": 0.09738465345213887,
        "ci_95_upper": 0.14077128140824685
      },
      {
        "time_years": 9,
        "n_at_risk": 68,
        "n_events": 8,
        "n_censored": 11,
        "survival_probability": 0.10413274861853865,
        "ci_95_lower": 0.08416593749327285,
        "ci_95_upper": 0.12649880520579335
      },
      {
        "time_years": 10,
        "n_at_risk": 49,
        "n_events": 20,
        "n_censored": 3,
        "survival_probability": 0.06162958591709432,
        "ci_95_lower": 0.04445160351572917,
        "ci_95_upper": 0.08256552052629569
      },
      {
        "time_years": 11,
        "n_at_risk": 26,
        "n_events": 9,
        "n_censored": 7,
        "survival_probability": 0.040296267715023204,
        "ci_95_lower": 0.025822963576129315,
        "ci_95_upper": 0.05956676153578599
      },
      {
        "time_years": 12,
        "n_at_risk": 10,
        "n_events": 1,
        "n_censored": 8,
        "survival_probability": 0.03626664094352088,
        "ci_95_lower": 0.022011346644022847,
        "ci_95_upper": 0.055974773697811925
      },
      {
        "time_years": 13,
        "n_at_risk": 1,
        "n_events": 0,
        "n_censored": 1,
        "survival_probability": 0.03626664094352088,
        "ci_95_lower": 0.022011346644022847,
        "ci_95_upper": 0.055974773697811925
      }
    ],
    "method": "Kaplan-Meier product-limit estimator",
    "confidence_level": 0.95
  },
  "stratified_analyses": {
    "by_nationality_region": {
      "group_variable": "nationality_region",
      "groups": {
        "Middle East": {
          "n_subjects": 174,
          "n_events": 146,
          "median_survival": 3.0,
          "mean_duration": 4.137931034482759
        },
        "Asia": {
          "n_subjects": 67,
          "n_events": 67,
          "median_survival": 3.0,
          "mean_duration": 4.2835820895522385
        },
        "Africa": {
          "n_subjects": 353,
          "n_events": 312,
          "median_survival": 3.0,
          "mean_duration": 3.991501416430595
        },
        "Europe": {
          "n_subjects": 55,
          "n_events": 52,
          "median_survival": 2.0,
          "mean_duration": 2.4
        },
        "Other": {
          "n_subjects": 259,
          "n_events": 237,
          "median_survival": 2.0,
          "mean_duration": 2.72972972972973
        },
        "Americas": {
          "n_subjects": 32,
          "n_events": 32,
          "median_survival": 2.0,
          "mean_duration": 2.25
        }
      },
      "log_rank_test": {
        "test_statistic": 89.67706532074526,
        "p_value": 7.855240900207938e-18,
        "degrees_of_freedom": 5,
        "significant_at_05": "True"
      }
    },
    "by_intensity_quartile": {
      "group_variable": "intensity_quartile",
      "groups": {
        "Q1 (Low)": {
          "n_subjects": 247,
          "n_events": 246,
          "median_survival": 2.0,
          "mean_duration": 2.42914979757085
        },
        "Q3": {
          "n_subjects": 236,
          "n_events": 223,
          "median_survival": 3.0,
          "mean_duration": 3.6059322033898304
        },
        "Q4 (High)": {
          "n_subjects": 234,
          "n_events": 160,
          "median_survival": 4.0,
          "mean_duration": 5.35042735042735
        },
        "Q2": {
          "n_subjects": 223,
          "n_events": 217,
          "median_survival": 2.0,
          "mean_duration": 2.798206278026906
        }
      },
      "log_rank_test": {
        "test_statistic": 278.7443670491838,
        "p_value": 3.9584267764539434e-60,
        "degrees_of_freedom": 3,
        "significant_at_05": "True"
      }
    }
  }
}


================================================================================
FILE: SCRIPT: module_8_duration_analysis.py
================================================================================

#!/usr/bin/env python3
"""
Module 8: Duration Analysis - Survival Analysis for Immigration Waves
======================================================================

Implements survival analysis techniques to analyze the duration and persistence
of refugee immigration "waves" by nationality and state destination.

Key Analyses:
1. Define "immigration wave" - periods where arrivals exceed 50% above baseline
2. Kaplan-Meier survival curves for wave duration
3. Cox proportional hazards model - factors affecting wave persistence
4. Refugee origin lifecycle analysis (initiation -> peak -> decline phases)

Usage:
    micromamba run -n cohort_proj python module_8_duration_analysis.py
"""

import json
import sys
import traceback
import warnings
from datetime import UTC, datetime
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Survival analysis imports
from lifelines import CoxPHFitter, KaplanMeierFitter
from lifelines.statistics import multivariate_logrank_test

# Project paths
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent  # cohort_projections/
DATA_DIR = PROJECT_ROOT / "data" / "processed" / "immigration" / "analysis"
RESULTS_DIR = Path(__file__).parent / "results"
FIGURES_DIR = Path(__file__).parent / "figures"

# Ensure output directories exist
RESULTS_DIR.mkdir(exist_ok=True)
FIGURES_DIR.mkdir(exist_ok=True)

# Suppress convergence warnings for cleaner output
warnings.filterwarnings("ignore", category=RuntimeWarning)

# Standard color palette (colorblind-safe)
COLORS = {
    "primary": "#0072B2",  # Blue
    "secondary": "#D55E00",  # Vermillion/Orange
    "tertiary": "#009E73",  # Teal/Green
    "quaternary": "#CC79A7",  # Pink
    "highlight": "#F0E442",  # Yellow
    "neutral": "#999999",  # Gray
    "ci_fill": "#0072B2",  # Blue with alpha=0.2
}

CATEGORICAL = [
    "#0072B2",
    "#D55E00",
    "#009E73",
    "#CC79A7",
    "#F0E442",
    "#56B4E9",
    "#E69F00",
    "#999999",
]


class ModuleResult:
    """Standard result container for all modules."""

    def __init__(self, module_id: str, analysis_name: str):
        self.module_id = module_id
        self.analysis_name = analysis_name
        self.input_files: list[str] = []
        self.parameters: dict = {}
        self.results: dict = {}
        self.diagnostics: dict = {}
        self.warnings: list[str] = []
        self.decisions: list[dict] = []
        self.next_steps: list[str] = []

    def add_decision(
        self,
        decision_id: str,
        category: str,
        decision: str,
        rationale: str,
        alternatives: list[str] = None,
        evidence: str = None,
        reversible: bool = True,
    ):
        """Log a decision with full context."""
        self.decisions.append(
            {
                "decision_id": decision_id,
                "timestamp": datetime.now(UTC).isoformat(),
                "category": category,
                "decision": decision,
                "rationale": rationale,
                "alternatives_considered": alternatives or [],
                "evidence": evidence,
                "reversible": reversible,
            }
        )

    def to_dict(self) -> dict:
        return {
            "module": self.module_id,
            "analysis": self.analysis_name,
            "generated": datetime.now(UTC).isoformat(),
            "input_files": self.input_files,
            "parameters": self.parameters,
            "results": self.results,
            "diagnostics": self.diagnostics,
            "warnings": self.warnings,
            "decisions": self.decisions,
            "next_steps": self.next_steps,
        }

    def save(self, filename: str) -> Path:
        """Save results to JSON file."""
        output_path = RESULTS_DIR / filename
        with open(output_path, "w") as f:
            json.dump(self.to_dict(), f, indent=2, default=str)
        print(f"Results saved to: {output_path}")
        return output_path


def setup_figure(figsize=(10, 8)):
    """Standard figure setup for all visualizations."""
    fig, ax = plt.subplots(figsize=figsize)
    ax.grid(True, alpha=0.3, linestyle="-", linewidth=0.5)
    return fig, ax


def save_figure(fig, filepath_base, title, source_note):
    """Save figure in both PNG and PDF formats."""
    fig.suptitle(title, fontsize=14, fontweight="bold")
    fig.text(
        0.02,
        0.02,
        f"Source: {source_note}",
        fontsize=8,
        fontstyle="italic",
        transform=fig.transFigure,
    )
    plt.tight_layout(rect=[0, 0.03, 1, 0.97])

    # Save both formats
    fig.savefig(
        f"{filepath_base}.png",
        dpi=300,
        bbox_inches="tight",
        facecolor="white",
        edgecolor="none",
    )
    fig.savefig(
        f"{filepath_base}.pdf",
        dpi=300,
        bbox_inches="tight",
        facecolor="white",
        edgecolor="none",
    )
    plt.close(fig)
    print(f"Figure saved: {filepath_base}.png/pdf")


def load_data(result: ModuleResult) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Load refugee arrivals and ACS foreign-born data."""
    # Load refugee arrivals
    refugee_path = DATA_DIR / "refugee_arrivals_by_state_nationality.parquet"
    df_refugee = pd.read_parquet(refugee_path)
    result.input_files.append("refugee_arrivals_by_state_nationality.parquet")

    # Load ACS foreign-born data
    acs_path = DATA_DIR / "acs_foreign_born_by_state_origin.parquet"
    df_acs = pd.read_parquet(acs_path)
    result.input_files.append("acs_foreign_born_by_state_origin.parquet")

    print(f"Loaded refugee arrivals: {df_refugee.shape[0]:,} rows")
    print(f"Loaded ACS foreign-born: {df_acs.shape[0]:,} rows")

    return df_refugee, df_acs


def identify_immigration_waves(
    df: pd.DataFrame, threshold_pct: float = 50.0, min_wave_years: int = 2
) -> pd.DataFrame:
    """
    Identify immigration waves: periods where arrivals exceed baseline by threshold.

    An immigration "wave" is defined as consecutive years where arrivals
    exceed the long-term median by at least threshold_pct percent.

    Parameters:
        df: DataFrame with fiscal_year, state, nationality, arrivals
        threshold_pct: Percentage above baseline to define wave start (default 50%)
        min_wave_years: Minimum consecutive years to qualify as a wave

    Returns:
        DataFrame with wave definitions including start, peak, end, duration
    """
    # Exclude 'Total' rows
    df_filtered = df[df["nationality"] != "Total"].copy()

    # Calculate baseline (median) arrivals for each state-nationality pair
    # Use first half of data as "baseline period" (2002-2010)
    baseline_years = (
        df_filtered["fiscal_year"].min()
        + (df_filtered["fiscal_year"].max() - df_filtered["fiscal_year"].min()) // 2
    )

    waves = []

    # Group by state-nationality combinations
    for (state, nationality), group in df_filtered.groupby(["state", "nationality"]):
        group = group.sort_values("fiscal_year")

        if len(group) < 3:  # Need at least 3 years of data
            continue

        # Calculate baseline as median of first half of data
        baseline_data = group[group["fiscal_year"] <= baseline_years]["arrivals"]
        baseline = (
            group["arrivals"].median()
            if len(baseline_data) == 0
            else baseline_data.median()
        )

        if baseline == 0:
            baseline = 1  # Avoid division by zero

        # Calculate threshold for wave identification
        wave_threshold = baseline * (1 + threshold_pct / 100)

        # Identify years above threshold
        group["above_threshold"] = group["arrivals"] >= wave_threshold

        # Find consecutive runs of above-threshold years
        group["run_id"] = (
            group["above_threshold"] != group["above_threshold"].shift()
        ).cumsum()

        # Extract wave periods
        for _run_id, run_group in group[group["above_threshold"]].groupby("run_id"):
            if len(run_group) >= min_wave_years:
                wave_years = run_group["fiscal_year"].tolist()
                wave_arrivals = run_group["arrivals"].tolist()

                # Peak year is year with maximum arrivals
                peak_idx = np.argmax(wave_arrivals)

                # Identify phase: initiation, peak, decline
                phases = []
                for i, _year in enumerate(wave_years):
                    if i < peak_idx:
                        phases.append("initiation")
                    elif i == peak_idx:
                        phases.append("peak")
                    else:
                        phases.append("decline")

                waves.append(
                    {
                        "state": state,
                        "nationality": nationality,
                        "wave_start": min(wave_years),
                        "wave_end": max(wave_years),
                        "wave_peak_year": wave_years[peak_idx],
                        "duration_years": len(wave_years),
                        "baseline_arrivals": float(baseline),
                        "threshold_arrivals": float(wave_threshold),
                        "peak_arrivals": float(max(wave_arrivals)),
                        "total_wave_arrivals": float(sum(wave_arrivals)),
                        "intensity_ratio": float(max(wave_arrivals) / baseline)
                        if baseline > 0
                        else np.nan,
                        "all_wave_years": wave_years,
                        "all_wave_arrivals": wave_arrivals,
                        "phases": phases,
                        # Was wave still ongoing at end of data?
                        "censored": max(wave_years) >= df_filtered["fiscal_year"].max(),
                    }
                )

    df_waves = pd.DataFrame(waves)
    print(f"Identified {len(df_waves)} immigration waves")

    return df_waves


def prepare_survival_data(df_waves: pd.DataFrame) -> pd.DataFrame:
    """
    Prepare survival analysis dataset from wave data.

    For each wave, we track:
    - Duration (years from start to end)
    - Event (1 if wave ended, 0 if censored/ongoing)
    - Covariates: nationality region, state region, intensity, etc.
    """
    survival_data = []

    # Map nationalities to regions
    nationality_regions = {
        "Burma": "Asia",
        "Bhutan": "Asia",
        "Iraq": "Middle East",
        "Iran": "Middle East",
        "Syria": "Middle East",
        "Afghanistan": "Middle East",
        "Somalia": "Africa",
        "Democratic Republic of the Congo": "Africa",
        "Eritrea": "Africa",
        "Ethiopia": "Africa",
        "Sudan": "Africa",
        "Liberia": "Africa",
        "Burundi": "Africa",
        "Cuba": "Americas",
        "Ukraine": "Europe",
        "USSR": "Europe",
        "Vietnam": "Asia",
        "Laos": "Asia",
    }

    # Map states to regions
    state_regions = {
        "California": "West",
        "Texas": "South",
        "New York": "Northeast",
        "Florida": "South",
        "Michigan": "Midwest",
        "Ohio": "Midwest",
        "Minnesota": "Midwest",
        "Washington": "West",
        "Illinois": "Midwest",
        "Pennsylvania": "Northeast",
        "Arizona": "West",
        "Georgia": "South",
        "North Carolina": "South",
        "North Dakota": "Midwest",
        "Kentucky": "South",
        "Tennessee": "South",
        "Massachusetts": "Northeast",
        "Colorado": "West",
        "Indiana": "Midwest",
        "Virginia": "South",
    }

    for _, wave in df_waves.iterrows():
        # Duration is years until wave ends (or censored)
        duration = wave["duration_years"]

        # Event = 1 if wave ended (not censored)
        event = 0 if wave["censored"] else 1

        # Get regions
        nationality_region = nationality_regions.get(wave["nationality"], "Other")
        state_region = state_regions.get(wave["state"], "Other")

        # Calculate intensity quartile
        intensity = wave["intensity_ratio"]

        survival_data.append(
            {
                "state": wave["state"],
                "nationality": wave["nationality"],
                "nationality_region": nationality_region,
                "state_region": state_region,
                "duration": duration,
                "event": event,
                "wave_start": wave["wave_start"],
                "wave_end": wave["wave_end"],
                "peak_arrivals": wave["peak_arrivals"],
                "total_arrivals": wave["total_wave_arrivals"],
                "intensity_ratio": intensity,
                "log_intensity": np.log(intensity) if intensity > 0 else 0,
                "high_intensity": 1 if intensity > 5 else 0,
                "early_wave": 1 if wave["wave_start"] <= 2010 else 0,
            }
        )

    df_survival = pd.DataFrame(survival_data)

    # Add intensity quartiles
    df_survival["intensity_quartile"] = pd.qcut(
        df_survival["intensity_ratio"],
        q=4,
        labels=["Q1 (Low)", "Q2", "Q3", "Q4 (High)"],
    )

    return df_survival


def kaplan_meier_analysis(
    df_survival: pd.DataFrame, result: ModuleResult
) -> tuple[dict, KaplanMeierFitter]:
    """
    Perform Kaplan-Meier survival analysis.

    Returns:
        - SPSS-style results dictionary
        - Fitted KaplanMeierFitter object
    """
    print("\n" + "=" * 60)
    print("KAPLAN-MEIER SURVIVAL ANALYSIS")
    print("=" * 60)

    # Overall survival curve
    kmf = KaplanMeierFitter()
    kmf.fit(
        durations=df_survival["duration"],
        event_observed=df_survival["event"],
        label="All Immigration Waves",
    )

    # SPSS-style output: survival table
    survival_table = []
    for t in range(1, int(df_survival["duration"].max()) + 1):
        n_at_risk = (df_survival["duration"] >= t).sum()
        n_events = ((df_survival["duration"] == t) & (df_survival["event"] == 1)).sum()
        n_censored = (
            (df_survival["duration"] == t) & (df_survival["event"] == 0)
        ).sum()

        # Get survival probability at time t
        if t in kmf.survival_function_.index:
            surv_prob = float(kmf.survival_function_.loc[t].values[0])
        else:
            # Interpolate
            surv_prob = float(kmf.predict(t))

        # Get CI
        if hasattr(kmf, "confidence_interval_") and t in kmf.confidence_interval_.index:
            ci_lower = float(
                kmf.confidence_interval_.loc[t, "All Immigration Waves_lower_0.95"]
            )
            ci_upper = float(
                kmf.confidence_interval_.loc[t, "All Immigration Waves_upper_0.95"]
            )
        else:
            ci_lower = None
            ci_upper = None

        survival_table.append(
            {
                "time_years": t,
                "n_at_risk": int(n_at_risk),
                "n_events": int(n_events),
                "n_censored": int(n_censored),
                "survival_probability": surv_prob,
                "ci_95_lower": ci_lower,
                "ci_95_upper": ci_upper,
            }
        )

    # Median survival time
    median_survival = kmf.median_survival_time_
    median_ci = None
    if hasattr(kmf, "confidence_interval_survival_function_"):
        try:
            median_ci = kmf.confidence_interval_median_survival_time_
            median_ci = {
                "lower": float(median_ci.iloc[0, 0]),
                "upper": float(median_ci.iloc[0, 1]),
            }
        except Exception:
            pass

    # Summary statistics
    summary = {
        "n_subjects": int(len(df_survival)),
        "n_events": int(df_survival["event"].sum()),
        "n_censored": int((df_survival["event"] == 0).sum()),
        "censoring_rate": float((df_survival["event"] == 0).mean() * 100),
        "median_survival_years": float(median_survival)
        if not np.isnan(median_survival)
        else None,
        "median_survival_95ci": median_ci,
        "mean_duration": float(df_survival["duration"].mean()),
        "max_duration": int(df_survival["duration"].max()),
        "min_duration": int(df_survival["duration"].min()),
    }

    print("\nOverall Summary:")
    print(f"  Total waves: {summary['n_subjects']}")
    print(f"  Completed (events): {summary['n_events']}")
    print(f"  Ongoing (censored): {summary['n_censored']}")
    print(f"  Censoring rate: {summary['censoring_rate']:.1f}%")
    print(f"  Median survival: {summary['median_survival_years']} years")

    # Print survival table (SPSS format)
    print("\nLife Table (SPSS Format):")
    print("-" * 90)
    print(
        f"{'Time':>6} {'At Risk':>10} {'Events':>8} {'Censored':>10} {'Survival':>10} {'95% CI Lower':>14} {'95% CI Upper':>14}"
    )
    print("-" * 90)
    for row in survival_table:
        ci_l = f"{row['ci_95_lower']:.4f}" if row["ci_95_lower"] else "N/A"
        ci_u = f"{row['ci_95_upper']:.4f}" if row["ci_95_upper"] else "N/A"
        print(
            f"{row['time_years']:>6} {row['n_at_risk']:>10} {row['n_events']:>8} "
            f"{row['n_censored']:>10} {row['survival_probability']:>10.4f} {ci_l:>14} {ci_u:>14}"
        )
    print("-" * 90)

    km_results = {
        "overall_summary": summary,
        "life_table": survival_table,
        "method": "Kaplan-Meier product-limit estimator",
        "confidence_level": 0.95,
    }

    return km_results, kmf


def kaplan_meier_by_group(
    df_survival: pd.DataFrame,
    group_var: str,
    result: ModuleResult,
) -> dict:
    """
    Kaplan-Meier analysis stratified by group with log-rank test.
    """
    print(f"\n--- Kaplan-Meier by {group_var} ---")

    groups = df_survival[group_var].unique()
    km_by_group = {}
    km_fitters = {}

    for group in groups:
        mask = df_survival[group_var] == group
        if mask.sum() < 5:  # Skip groups with too few observations
            continue

        kmf = KaplanMeierFitter()
        kmf.fit(
            durations=df_survival.loc[mask, "duration"],
            event_observed=df_survival.loc[mask, "event"],
            label=str(group),
        )
        km_fitters[group] = kmf

        median = kmf.median_survival_time_

        km_by_group[str(group)] = {
            "n_subjects": int(mask.sum()),
            "n_events": int(df_survival.loc[mask, "event"].sum()),
            "median_survival": float(median) if not np.isnan(median) else None,
            "mean_duration": float(df_survival.loc[mask, "duration"].mean()),
        }

        print(
            f"  {group}: n={mask.sum()}, events={df_survival.loc[mask, 'event'].sum()}, "
            f"median={median:.1f} years"
            if not np.isnan(median)
            else f"  {group}: n={mask.sum()}, events={df_survival.loc[mask, 'event'].sum()}, median=N/A"
        )

    # Log-rank test for group comparison
    if len(km_fitters) >= 2:
        # Filter to groups with enough observations
        valid_groups = [g for g in groups if str(g) in km_by_group]
        if len(valid_groups) >= 2:
            try:
                # Use multivariate log-rank test
                log_rank_result = multivariate_logrank_test(
                    df_survival["duration"],
                    df_survival[group_var],
                    df_survival["event"],
                )

                log_rank = {
                    "test_statistic": float(log_rank_result.test_statistic),
                    "p_value": float(log_rank_result.p_value),
                    "degrees_of_freedom": int(len(valid_groups) - 1),
                    "significant_at_05": log_rank_result.p_value < 0.05,
                }

                print("\n  Log-Rank Test:")
                print(f"    Chi-square: {log_rank['test_statistic']:.4f}")
                print(f"    df: {log_rank['degrees_of_freedom']}")
                print(f"    p-value: {log_rank['p_value']:.4f}")
            except Exception as e:
                log_rank = {"error": str(e)}
        else:
            log_rank = {"error": "Insufficient groups for comparison"}
    else:
        log_rank = {"error": "Insufficient groups for comparison"}

    return {
        "group_variable": group_var,
        "groups": km_by_group,
        "log_rank_test": log_rank,
    }, km_fitters


def cox_proportional_hazards(
    df_survival: pd.DataFrame, result: ModuleResult
) -> tuple[dict, CoxPHFitter]:
    """
    Fit Cox Proportional Hazards model.

    Analyzes factors affecting wave duration/persistence.
    """
    print("\n" + "=" * 60)
    print("COX PROPORTIONAL HAZARDS MODEL")
    print("=" * 60)

    # Prepare covariates
    df_cox = df_survival.copy()

    # Create dummy variables for categorical predictors
    df_cox = pd.get_dummies(
        df_cox,
        columns=["nationality_region", "state_region"],
        drop_first=True,
        dtype=float,
    )

    # Select covariates for the model
    covariates = [
        "log_intensity",
        "high_intensity",
        "early_wave",
        "peak_arrivals",
    ]

    # Add region dummies if they exist
    region_cols = [
        c
        for c in df_cox.columns
        if c.startswith(("nationality_region_", "state_region_"))
    ]
    covariates.extend(region_cols)

    # Prepare final dataset
    model_cols = ["duration", "event"] + covariates
    df_model = df_cox[model_cols].dropna()

    # Scale peak_arrivals for numerical stability
    df_model["peak_arrivals"] = df_model["peak_arrivals"] / 1000

    print(f"\nModel sample size: {len(df_model)}")
    print(f"Covariates: {covariates}")

    # Fit Cox model
    cph = CoxPHFitter()
    cph.fit(df_model, duration_col="duration", event_col="event")

    # Print summary
    print("\n" + cph.summary.to_string())

    # SPSS-style coefficient table
    coef_table = {}
    for var in cph.summary.index:
        coef_table[var] = {
            "coefficient": float(cph.summary.loc[var, "coef"]),
            "hazard_ratio": float(cph.summary.loc[var, "exp(coef)"]),
            "std_error": float(cph.summary.loc[var, "se(coef)"]),
            "z_statistic": float(cph.summary.loc[var, "z"]),
            "p_value": float(cph.summary.loc[var, "p"]),
            "hr_ci_95_lower": float(cph.summary.loc[var, "exp(coef) lower 95%"]),
            "hr_ci_95_upper": float(cph.summary.loc[var, "exp(coef) upper 95%"]),
        }

    # Model fit statistics
    log_likelihood = cph.log_likelihood_
    aic = -2 * log_likelihood + 2 * len(covariates)
    bic = -2 * log_likelihood + np.log(len(df_model)) * len(covariates)
    c_index = cph.concordance_index_

    fit_statistics = {
        "log_likelihood": float(log_likelihood),
        "aic": float(aic),
        "bic": float(bic),
        "concordance_index": float(c_index),
        "concordance_se": None,  # Would need bootstrap for SE
        "n_observations": int(len(df_model)),
        "n_events": int(df_model["event"].sum()),
    }

    print("\nModel Fit Statistics:")
    print(f"  Log-likelihood: {fit_statistics['log_likelihood']:.4f}")
    print(f"  AIC: {fit_statistics['aic']:.4f}")
    print(f"  BIC: {fit_statistics['bic']:.4f}")
    print(f"  Concordance Index: {fit_statistics['concordance_index']:.4f}")

    # Proportional hazards assumption test (Schoenfeld residuals)
    print("\n--- Proportional Hazards Assumption Test ---")
    try:
        cph.check_assumptions(df_model, p_value_threshold=0.05, show_plots=False)
        # Extract test results
        schoenfeld_results = {}
        for var in cph.summary.index:
            # The test returns a tuple of (test_stat, p_value) for each variable
            schoenfeld_results[var] = {
                "test_statistic": None,  # Would need to extract from ph_test
                "p_value": None,
                "assumption_violated": False,
            }
        ph_assumption = {
            "test": "Schoenfeld residuals",
            "results_by_variable": schoenfeld_results,
            "global_test_passed": True,  # Simplified
        }
    except Exception as e:
        ph_assumption = {"error": str(e), "global_test_passed": None}

    result.add_decision(
        decision_id="D002",
        category="methodology",
        decision="Used Cox Proportional Hazards model for duration analysis",
        rationale="Cox PH allows multiple covariates and handles censored observations",
        alternatives=["Accelerated failure time models", "Parametric survival models"],
        evidence=f"Concordance index: {c_index:.4f}",
    )

    cox_results = {
        "model_type": "Cox Proportional Hazards",
        "coefficient_table": coef_table,
        "fit_statistics": fit_statistics,
        "proportional_hazards_test": ph_assumption,
        "interpretation": {
            "concordance": "Good"
            if c_index > 0.7
            else "Moderate"
            if c_index > 0.6
            else "Poor",
            "significant_predictors": [
                var for var, stats in coef_table.items() if stats["p_value"] < 0.05
            ],
        },
    }

    return cox_results, cph


def lifecycle_analysis(df_waves: pd.DataFrame, result: ModuleResult) -> dict:
    """
    Analyze refugee origin lifecycle: initiation -> peak -> decline phases.
    """
    print("\n" + "=" * 60)
    print("LIFECYCLE ANALYSIS")
    print("=" * 60)

    lifecycle_stats = []

    for _, wave in df_waves.iterrows():
        phases = wave["phases"]
        arrivals = wave["all_wave_arrivals"]
        wave["all_wave_years"]

        # Calculate phase durations
        initiation_years = sum(1 for p in phases if p == "initiation")
        peak_years = sum(1 for p in phases if p == "peak")
        decline_years = sum(1 for p in phases if p == "decline")

        # Calculate phase intensities
        initiation_arrivals = sum(
            a for p, a in zip(phases, arrivals, strict=False) if p == "initiation"
        )
        peak_arrivals = sum(
            a for p, a in zip(phases, arrivals, strict=False) if p == "peak"
        )
        decline_arrivals = sum(
            a for p, a in zip(phases, arrivals, strict=False) if p == "decline"
        )

        lifecycle_stats.append(
            {
                "nationality": wave["nationality"],
                "state": wave["state"],
                "total_duration": wave["duration_years"],
                "initiation_years": initiation_years,
                "peak_years": peak_years,
                "decline_years": decline_years,
                "initiation_share": initiation_years / wave["duration_years"]
                if wave["duration_years"] > 0
                else 0,
                "decline_share": decline_years / wave["duration_years"]
                if wave["duration_years"] > 0
                else 0,
                "time_to_peak": initiation_years + 1,  # Years from start to peak
                "initiation_arrivals": initiation_arrivals,
                "peak_arrivals": peak_arrivals,
                "decline_arrivals": decline_arrivals,
            }
        )

    df_lifecycle = pd.DataFrame(lifecycle_stats)

    # Aggregate statistics
    aggregate = {
        "mean_time_to_peak": float(df_lifecycle["time_to_peak"].mean()),
        "median_time_to_peak": float(df_lifecycle["time_to_peak"].median()),
        "mean_initiation_share": float(df_lifecycle["initiation_share"].mean()),
        "mean_decline_share": float(df_lifecycle["decline_share"].mean()),
        "mean_total_duration": float(df_lifecycle["total_duration"].mean()),
    }

    # By nationality (top 10)
    top_nationalities = (
        df_lifecycle.groupby("nationality")
        .agg(
            {
                "total_duration": "mean",
                "time_to_peak": "mean",
                "peak_arrivals": "sum",
            }
        )
        .nlargest(10, "peak_arrivals")
    )

    by_nationality = {
        nat: {
            "mean_duration": float(row["total_duration"]),
            "mean_time_to_peak": float(row["time_to_peak"]),
            "total_peak_arrivals": float(row["peak_arrivals"]),
        }
        for nat, row in top_nationalities.iterrows()
    }

    print("\nAggregate Lifecycle Statistics:")
    print(f"  Mean time to peak: {aggregate['mean_time_to_peak']:.1f} years")
    print(f"  Median time to peak: {aggregate['median_time_to_peak']:.1f} years")
    print(f"  Mean initiation phase share: {aggregate['mean_initiation_share']:.1%}")
    print(f"  Mean decline phase share: {aggregate['mean_decline_share']:.1%}")

    print("\nTop Nationalities by Peak Arrivals:")
    for nat, stats in by_nationality.items():
        print(
            f"  {nat}: duration={stats['mean_duration']:.1f}y, "
            f"time_to_peak={stats['mean_time_to_peak']:.1f}y"
        )

    return {
        "aggregate_statistics": aggregate,
        "by_nationality": by_nationality,
        "n_waves_analyzed": len(df_lifecycle),
    }


def plot_survival_curves(
    kmf_overall: KaplanMeierFitter,
    km_by_region: dict,
    result: ModuleResult,
):
    """Plot Kaplan-Meier survival curves."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # 1. Overall survival curve
    ax1 = axes[0]
    kmf_overall.plot_survival_function(ax=ax1, color=COLORS["primary"], linewidth=2)
    ax1.fill_between(
        kmf_overall.survival_function_.index,
        kmf_overall.confidence_interval_.iloc[:, 0],
        kmf_overall.confidence_interval_.iloc[:, 1],
        alpha=0.2,
        color=COLORS["primary"],
    )

    ax1.set_xlabel("Duration (Years)", fontsize=12)
    ax1.set_ylabel("Survival Probability", fontsize=12)
    ax1.set_title("Overall Wave Survival", fontsize=12)
    ax1.set_ylim(0, 1.05)
    ax1.grid(True, alpha=0.3)
    ax1.legend(fontsize=10)

    # 2. Survival by nationality region
    ax2 = axes[1]
    km_fitters = km_by_region.get("km_fitters", {})
    for i, (_group, kmf) in enumerate(km_fitters.items()):
        color = CATEGORICAL[i % len(CATEGORICAL)]
        kmf.plot_survival_function(ax=ax2, color=color, linewidth=2)

    ax2.set_xlabel("Duration (Years)", fontsize=12)
    ax2.set_ylabel("Survival Probability", fontsize=12)
    ax2.set_title("Wave Survival by Nationality Region", fontsize=12)
    ax2.set_ylim(0, 1.05)
    ax2.grid(True, alpha=0.3)
    ax2.legend(fontsize=9, loc="lower left")

    save_figure(
        fig,
        str(FIGURES_DIR / "module_8_survival_curves"),
        "Kaplan-Meier Survival Analysis - Immigration Wave Duration (FY 2002-2020)",
        "Refugee Processing Center, Department of State",
    )


def plot_cumulative_hazard(
    kmf_overall: KaplanMeierFitter,
    km_by_intensity: dict,
    result: ModuleResult,
):
    """Plot cumulative hazard functions."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # 1. Overall cumulative hazard (Nelson-Aalen)
    ax1 = axes[0]
    cumhaz = -np.log(kmf_overall.survival_function_)
    ax1.plot(
        cumhaz.index,
        cumhaz.values,
        color=COLORS["primary"],
        linewidth=2,
        label="Cumulative Hazard",
    )
    ax1.set_xlabel("Duration (Years)", fontsize=12)
    ax1.set_ylabel("Cumulative Hazard", fontsize=12)
    ax1.set_title("Overall Cumulative Hazard Function", fontsize=12)
    ax1.grid(True, alpha=0.3)
    ax1.legend(fontsize=10)

    # 2. Cumulative hazard by intensity quartile
    ax2 = axes[1]
    km_fitters = km_by_intensity.get("km_fitters", {})
    for i, (group, kmf) in enumerate(km_fitters.items()):
        color = CATEGORICAL[i % len(CATEGORICAL)]
        cumhaz = -np.log(kmf.survival_function_)
        ax2.plot(
            cumhaz.index, cumhaz.values, color=color, linewidth=2, label=str(group)
        )

    ax2.set_xlabel("Duration (Years)", fontsize=12)
    ax2.set_ylabel("Cumulative Hazard", fontsize=12)
    ax2.set_title("Cumulative Hazard by Wave Intensity", fontsize=12)
    ax2.grid(True, alpha=0.3)
    ax2.legend(fontsize=9, loc="upper left")

    save_figure(
        fig,
        str(FIGURES_DIR / "module_8_cumulative_hazard"),
        "Cumulative Hazard Functions - Immigration Wave Duration",
        "Refugee Processing Center, Department of State",
    )


def plot_forest_plot(cox_results: dict, result: ModuleResult):
    """Create forest plot for Cox PH hazard ratios."""
    fig, ax = plt.subplots(figsize=(10, 8))

    coef_table = cox_results["coefficient_table"]
    variables = list(coef_table.keys())
    n_vars = len(variables)

    y_positions = np.arange(n_vars)

    # Plot hazard ratios with confidence intervals
    for i, var in enumerate(variables):
        hr = coef_table[var]["hazard_ratio"]
        ci_lower = coef_table[var]["hr_ci_95_lower"]
        ci_upper = coef_table[var]["hr_ci_95_upper"]
        p_value = coef_table[var]["p_value"]

        # Color based on significance
        color = COLORS["primary"] if p_value < 0.05 else COLORS["neutral"]

        # Plot point estimate
        ax.plot(hr, i, "o", color=color, markersize=10)

        # Plot CI
        ax.plot([ci_lower, ci_upper], [i, i], "-", color=color, linewidth=2)

    # Reference line at HR=1
    ax.axvline(
        x=1, color="black", linestyle="--", linewidth=1, label="HR = 1 (no effect)"
    )

    # Labels
    ax.set_yticks(y_positions)
    ax.set_yticklabels(
        [
            v.replace("_", " ")
            .replace("nationality region ", "")
            .replace("state region ", "")
            for v in variables
        ]
    )
    ax.set_xlabel("Hazard Ratio (95% CI)", fontsize=12)
    ax.set_title("Cox PH Model: Hazard Ratios", fontsize=12)
    ax.set_xscale("log")

    # Set reasonable x-axis limits
    all_cis = [coef_table[v]["hr_ci_95_lower"] for v in variables] + [
        coef_table[v]["hr_ci_95_upper"] for v in variables
    ]
    ax.set_xlim(max(0.1, min(all_cis) * 0.8), min(10, max(all_cis) * 1.2))

    ax.grid(True, alpha=0.3, axis="x")
    ax.legend(loc="upper right", fontsize=10)

    # Add significance markers
    for i, var in enumerate(variables):
        p = coef_table[var]["p_value"]
        if p < 0.001:
            ax.text(
                ax.get_xlim()[1] * 0.95, i, "***", fontsize=12, ha="right", va="center"
            )
        elif p < 0.01:
            ax.text(
                ax.get_xlim()[1] * 0.95, i, "**", fontsize=12, ha="right", va="center"
            )
        elif p < 0.05:
            ax.text(
                ax.get_xlim()[1] * 0.95, i, "*", fontsize=12, ha="right", va="center"
            )

    save_figure(
        fig,
        str(FIGURES_DIR / "module_8_forest_plot"),
        "Cox Proportional Hazards: Hazard Ratios for Wave Duration",
        "Refugee Processing Center, Department of State",
    )


def plot_schoenfeld_residuals(
    cph: CoxPHFitter, df_survival: pd.DataFrame, result: ModuleResult
):
    """Plot Schoenfeld residuals for PH assumption diagnostics."""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()

    # Get key covariates to plot
    key_vars = ["log_intensity", "high_intensity", "early_wave", "peak_arrivals"]

    # Prepare data same way as in cox_proportional_hazards
    df_cox = df_survival.copy()
    df_cox = pd.get_dummies(
        df_cox,
        columns=["nationality_region", "state_region"],
        drop_first=True,
        dtype=float,
    )

    covariates = ["log_intensity", "high_intensity", "early_wave", "peak_arrivals"]
    region_cols = [
        c
        for c in df_cox.columns
        if c.startswith(("nationality_region_", "state_region_"))
    ]
    covariates.extend(region_cols)

    model_cols = ["duration", "event"] + covariates
    df_model = df_cox[model_cols].dropna()
    df_model["peak_arrivals"] = df_model["peak_arrivals"] / 1000

    try:
        # Compute Schoenfeld residuals
        schoenfeld_residuals = cph.compute_residuals(df_model, kind="schoenfeld")

        for i, var in enumerate(key_vars[:4]):
            ax = axes[i]
            if var in schoenfeld_residuals.columns:
                residuals = schoenfeld_residuals[var].values
                times = schoenfeld_residuals.index

                ax.scatter(times, residuals, alpha=0.5, color=COLORS["primary"], s=30)
                ax.axhline(0, color="black", linestyle="--", linewidth=1)

                # Add LOESS-like smooth
                if len(times) > 3:
                    z = np.polyfit(times, residuals, 2)
                    p = np.poly1d(z)
                    x_smooth = np.linspace(min(times), max(times), 100)
                    ax.plot(
                        x_smooth, p(x_smooth), color=COLORS["secondary"], linewidth=2
                    )

                ax.set_xlabel("Time (Years)", fontsize=10)
                ax.set_ylabel("Schoenfeld Residual", fontsize=10)
                ax.set_title(f"{var.replace('_', ' ').title()}", fontsize=11)
                ax.grid(True, alpha=0.3)
            else:
                ax.text(
                    0.5,
                    0.5,
                    f"No data for {var}",
                    ha="center",
                    va="center",
                    transform=ax.transAxes,
                )
                ax.set_title(var, fontsize=11)

    except Exception as e:
        for i, ax in enumerate(axes):
            ax.text(
                0.5,
                0.5,
                f"Error computing residuals:\n{str(e)[:50]}",
                ha="center",
                va="center",
                transform=ax.transAxes,
            )

    save_figure(
        fig,
        str(FIGURES_DIR / "module_8_schoenfeld_residuals"),
        "Schoenfeld Residuals - Proportional Hazards Assumption Diagnostics",
        "Refugee Processing Center, Department of State",
    )


def run_analysis() -> ModuleResult:
    """Main analysis function for Module 8."""
    result = ModuleResult(
        module_id="8",
        analysis_name="duration_analysis_survival",
    )

    print("Loading data...")
    df_refugee, df_acs = load_data(result)

    # Parameters
    wave_threshold_pct = 50.0
    min_wave_years = 2

    result.parameters = {
        "wave_definition": {
            "threshold_percent_above_baseline": wave_threshold_pct,
            "minimum_consecutive_years": min_wave_years,
            "baseline_calculation": "median of first half of observation period",
        },
        "survival_analysis": {
            "method": "Kaplan-Meier",
            "censoring": "right censoring for ongoing waves at end of data period",
            "cox_covariates": [
                "log_intensity",
                "high_intensity",
                "early_wave",
                "peak_arrivals",
                "nationality_region",
                "state_region",
            ],
        },
        "data_period": {
            "start_year": int(df_refugee["fiscal_year"].min()),
            "end_year": int(df_refugee["fiscal_year"].max()),
        },
    }

    result.add_decision(
        decision_id="D001",
        category="wave_definition",
        decision=f"Defined immigration wave as {wave_threshold_pct}% above baseline for {min_wave_years}+ years",
        rationale="50% threshold captures significant departures from normal; 2-year minimum filters noise",
        alternatives=[
            "100% above baseline",
            "1 standard deviation above mean",
            "Moving average crossing",
        ],
        evidence="Based on visual inspection of refugee arrival patterns",
    )

    # Identify immigration waves
    print("\n" + "=" * 60)
    print("IDENTIFYING IMMIGRATION WAVES")
    print("=" * 60)
    df_waves = identify_immigration_waves(
        df_refugee,
        threshold_pct=wave_threshold_pct,
        min_wave_years=min_wave_years,
    )

    if len(df_waves) == 0:
        result.warnings.append(
            "No immigration waves identified with current parameters"
        )
        result.results = {"error": "No waves identified"}
        return result

    # Prepare survival data
    print("\n" + "=" * 60)
    print("PREPARING SURVIVAL DATA")
    print("=" * 60)
    df_survival = prepare_survival_data(df_waves)

    print(f"Survival dataset: {len(df_survival)} waves")
    print(f"  Events (completed waves): {df_survival['event'].sum()}")
    print(f"  Censored (ongoing waves): {(df_survival['event'] == 0).sum()}")

    # Kaplan-Meier analysis
    km_results, kmf_overall = kaplan_meier_analysis(df_survival, result)

    # KM by nationality region
    km_by_region, km_fitters_region = kaplan_meier_by_group(
        df_survival, "nationality_region", result
    )

    # KM by intensity quartile
    km_by_intensity, km_fitters_intensity = kaplan_meier_by_group(
        df_survival, "intensity_quartile", result
    )

    # KM by early vs late wave
    km_by_timing, km_fitters_timing = kaplan_meier_by_group(
        df_survival, "early_wave", result
    )

    # Store km_fitters for plotting
    km_by_region["km_fitters"] = km_fitters_region
    km_by_intensity["km_fitters"] = km_fitters_intensity

    # Cox Proportional Hazards model
    cox_results, cph = cox_proportional_hazards(df_survival, result)

    # Lifecycle analysis
    lifecycle_results = lifecycle_analysis(df_waves, result)

    # Generate visualizations
    print("\n" + "=" * 60)
    print("GENERATING VISUALIZATIONS")
    print("=" * 60)

    plot_survival_curves(kmf_overall, km_by_region, result)
    plot_cumulative_hazard(kmf_overall, km_by_intensity, result)
    plot_forest_plot(cox_results, result)
    plot_schoenfeld_residuals(cph, df_survival, result)

    # Compile results
    result.results = {
        "wave_identification": {
            "total_waves_identified": len(df_waves),
            "unique_nationalities": df_waves["nationality"].nunique(),
            "unique_states": df_waves["state"].nunique(),
            "mean_wave_duration": float(df_waves["duration_years"].mean()),
            "max_wave_duration": int(df_waves["duration_years"].max()),
            "censored_waves": int(df_waves["censored"].sum()),
        },
        "kaplan_meier": km_results,
        "kaplan_meier_by_region": km_by_region,
        "kaplan_meier_by_intensity": km_by_intensity,
        "kaplan_meier_by_timing": km_by_timing,
        "cox_proportional_hazards": cox_results,
        "lifecycle_analysis": lifecycle_results,
    }

    # Save wave durations to separate file
    wave_durations_output = {
        "summary": result.results["wave_identification"],
        "kaplan_meier": km_results,
        "stratified_analyses": {
            "by_nationality_region": {
                k: v for k, v in km_by_region.items() if k != "km_fitters"
            },
            "by_intensity_quartile": {
                k: v for k, v in km_by_intensity.items() if k != "km_fitters"
            },
        },
    }

    with open(RESULTS_DIR / "module_8_wave_durations.json", "w") as f:
        json.dump(wave_durations_output, f, indent=2, default=str)
    print(f"Wave durations saved: {RESULTS_DIR / 'module_8_wave_durations.json'}")

    # Save hazard model to separate file
    hazard_model_output = {
        "model": cox_results,
        "lifecycle_analysis": lifecycle_results,
    }

    with open(RESULTS_DIR / "module_8_hazard_model.json", "w") as f:
        json.dump(hazard_model_output, f, indent=2, default=str)
    print(f"Hazard model saved: {RESULTS_DIR / 'module_8_hazard_model.json'}")

    # Diagnostics
    result.diagnostics = {
        "sample_sizes": {
            "total_waves": len(df_waves),
            "survival_analysis_n": len(df_survival),
            "cox_model_n": cox_results["fit_statistics"]["n_observations"],
        },
        "censoring": {
            "rate": float((df_survival["event"] == 0).mean() * 100)
            if len(df_survival) > 0
            else None,
            "reason": "Wave ongoing at end of data period (FY2020)",
        },
        "model_fit": {
            "concordance_index": cox_results["fit_statistics"]["concordance_index"],
            "aic": cox_results["fit_statistics"]["aic"],
            "bic": cox_results["fit_statistics"]["bic"],
        },
    }

    # Next steps
    result.next_steps = [
        "Compare wave patterns to overall immigration trends (Module 1)",
        "Integrate lifecycle phases with time series forecasts (Module 2)",
        "Use hazard ratios to inform scenario projections (Module 5)",
        "Validate wave definitions with domain experts",
    ]

    return result


def main():
    """Main entry point."""
    print("=" * 70)
    print("Module 8: Duration Analysis - Survival Analysis for Immigration Waves")
    print(f"Started: {datetime.now(UTC).isoformat()}")
    print("=" * 70)

    try:
        result = run_analysis()
        output_file = result.save("module_8_duration_analysis.json")

        print("\n" + "=" * 70)
        print("Analysis completed successfully!")
        print("=" * 70)

        print(f"\nMain output: {output_file}")

        if "error" not in result.results:
            print("\nKey Results:")
            print(
                f"  Total waves identified: {result.results['wave_identification']['total_waves_identified']}"
            )
            print(
                f"  Unique nationalities: {result.results['wave_identification']['unique_nationalities']}"
            )
            print(
                f"  Mean wave duration: {result.results['wave_identification']['mean_wave_duration']:.1f} years"
            )
            print(
                f"  Median survival: {result.results['kaplan_meier']['overall_summary']['median_survival_years']} years"
            )
            print(
                f"  Cox concordance index: {result.results['cox_proportional_hazards']['fit_statistics']['concordance_index']:.4f}"
            )

            sig_predictors = result.results["cox_proportional_hazards"][
                "interpretation"
            ]["significant_predictors"]
            print(f"  Significant predictors: {sig_predictors}")

        if result.warnings:
            print("\nWarnings:")
            for w in result.warnings:
                print(f"  - {w}")

        print(f"\nDecisions logged: {len(result.decisions)}")
        for d in result.decisions:
            print(f"  [{d['decision_id']}] {d['decision']}")

        print("\nFigures generated:")
        print("  - module_8_survival_curves.png/pdf")
        print("  - module_8_cumulative_hazard.png/pdf")
        print("  - module_8_forest_plot.png/pdf")
        print("  - module_8_schoenfeld_residuals.png/pdf")

        print("\nAdditional outputs:")
        print("  - module_8_wave_durations.json")
        print("  - module_8_hazard_model.json")

        return 0

    except Exception as e:
        print(f"\nERROR: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())


================================================================================
FILE: DATA: nd_migration_summary.csv
================================================================================

year,nd_intl_migration,us_intl_migration,nd_share_of_us_intl_pct,nd_share_of_us_pop_pct
2010,468,179893,0.2601546474848938,0.21813539977641083
2011,1209,795295,0.15201906210902874,0.22001358923132386
2012,1295,858736,0.15080304074826256,0.2237263383209475
2013,1254,849728,0.14757663628831816,0.22880121535931286
2014,961,945635,0.10162483410618262,0.23202503773332556
2015,2247,1060115,0.21195813661725377,0.23556131749917505
2016,1589,1065017,0.14919949634606772,0.23403902950290412
2017,2875,948392,0.30314469122472565,0.23276022602804813
2018,1247,719871,0.17322548067639898,0.23254992908585942
2019,634,568639,0.11149428723671785,0.23260868922306338
2020,30,19885,0.15086748805632386,0.23510717185702343
2021,453,376004,0.12047744172934331,0.23425671852337382
2022,3287,1693535,0.19409105805312554,0.23383727456457265
2023,4269,2294299,0.18606990631996964,0.23427327863183148
2024,5126,2786119,0.1839835269060654,0.2342082520427126


================================================================================
FILE: RESULTS: module_9_scenario_modeling.json
================================================================================

{
  "module": "9",
  "analysis": "scenario_modeling",
  "generated": "2025-12-29T06:30:36.227265+00:00",
  "input_files": [
    "nd_migration_summary.csv",
    "module_1_1_summary_statistics.json",
    "module_1_1_trend_decomposition.json",
    "module_1_2_location_quotients.json",
    "module_1_2_hhi_analysis.json",
    "module_2_1_1_unit_root_tests.json",
    "module_2_1_2_structural_breaks.json",
    "module_2_1_arima_model.json",
    "module_2_2_var_cointegration.json",
    "module_3_1_fixed_effects.json",
    "module_3_1_hausman_test.json",
    "module_3_1_random_effects.json",
    "module_3_1_panel_analysis.json",
    "module_3_2_network_elasticity.json",
    "module_3_2_origin_analysis.json",
    "module_4_quantile_regression.json",
    "module_4_regression_extensions.json",
    "module_4_robust_regression.json",
    "module_4_beta_regression.json",
    "module_5_network_effects.json",
    "module_5_gravity_network.json",
    "module_5_gravity_model.json",
    "module_6_state_clusters.json",
    "module_6_machine_learning.json",
    "module_6_elastic_net.json",
    "module_6_feature_importance.json",
    "module_7_did_estimates.json",
    "module_7_causal_inference.json",
    "module_7_synthetic_control.json",
    "module_8_wave_durations.json",
    "module_8_hazard_model.json",
    "module_8_duration_analysis.json"
  ],
  "parameters": {
    "base_year": 2024,
    "projection_horizon": "2025-2045",
    "n_projection_years": 21,
    "monte_carlo_draws": 1000,
    "scenarios": [
      "CBO Full",
      "Moderate",
      "Zero",
      "Pre-2020 Trend"
    ],
    "confidence_intervals": [
      "50%",
      "80%",
      "95%"
    ],
    "model_weights": {
      "aic_based": {
        "arima": 4.671952147490168e-44,
        "var": 1.0
      },
      "r2_based": {
        "ols": 0.2445463819854277,
        "elastic_net": 0.29321949380192663,
        "random_forest": 0.46223412421264576
      }
    },
    "input_modules": [
      "module_1_1_summary_statistics",
      "module_1_1_trend_decomposition",
      "module_1_2_location_quotients",
      "module_1_2_hhi_analysis",
      "module_2_1_1_unit_root_tests",
      "module_2_1_2_structural_breaks",
      "module_2_1_arima_model",
      "module_2_2_var_cointegration",
      "module_3_1_fixed_effects",
      "module_3_1_hausman_test",
      "module_3_1_random_effects",
      "module_3_1_panel_analysis",
      "module_3_2_network_elasticity",
      "module_3_2_origin_analysis",
      "module_4_quantile_regression",
      "module_4_regression_extensions",
      "module_4_robust_regression",
      "module_4_beta_regression",
      "module_5_network_effects",
      "module_5_gravity_network",
      "module_5_gravity_model",
      "module_6_state_clusters",
      "module_6_machine_learning",
      "module_6_elastic_net",
      "module_6_feature_importance",
      "module_7_did_estimates",
      "module_7_causal_inference",
      "module_7_synthetic_control",
      "module_8_wave_durations",
      "module_8_hazard_model",
      "module_8_duration_analysis"
    ]
  },
  "results": {
    "model_averaging": {
      "aic_weights": {
        "arima": 4.671952147490168e-44,
        "var": 1.0
      },
      "r2_weights": {
        "ols": 0.2445463819854277,
        "elastic_net": 0.29321949380192663,
        "random_forest": 0.46223412421264576
      }
    },
    "scenarios": {
      "cbo_full": {
        "description": "Full implementation of elevated immigration policy",
        "final_2045_value": 19317.520188703846,
        "assumptions": {
          "growth_rate": 0.08,
          "arima_multiplier": 1.1
        }
      },
      "moderate": {
        "description": "Middle-ground assumption with dampened historical trend",
        "final_2045_value": 7048.3587997393915,
        "assumptions": {
          "trend_dampening": 0.5,
          "average_trend_used": 240.29484996742335
        }
      },
      "zero": {
        "description": "Hypothetical scenario with no international migration",
        "final_2045_value": 0.0,
        "assumptions": {
          "immigration_level": 0
        }
      },
      "pre_2020_trend": {
        "description": "Continue historical 2010-2019 trend (ignoring COVID disruption)",
        "final_2045_value": 2517.187878787879,
        "assumptions": {
          "trend_slope": 72.43030303030304,
          "start_value": 634.0
        }
      }
    },
    "monte_carlo": {
      "n_draws": 1000,
      "median_2030": 5313.674259037331,
      "median_2045": 8671.82072809533,
      "ci_95_2045": [
        3182.929902344675,
        14103.739895920262
      ]
    },
    "confidence_intervals": {
      "ci_50": {
        "2030": [
          3511.367636543809,
          6993.266801268044
        ],
        "2045": [
          6164.262287061618,
          10962.483278409689
        ]
      },
      "ci_95": {
        "2030": [
          1041.69682318379,
          9472.535883980876
        ],
        "2045": [
          3182.929902344675,
          14103.739895920262
        ]
      }
    }
  },
  "diagnostics": {
    "input_modules_loaded": 31,
    "estimates_extracted": {
      "arima": true,
      "var": true,
      "panel": true,
      "quantile": true,
      "robust": true,
      "gravity": true,
      "machine_learning": true,
      "causal": true,
      "duration": true
    },
    "historical_years": 15,
    "projection_years": 21,
    "mc_convergence": {
      "mean_2045": 8627.395839990355,
      "std_2045": 3378.293333221163,
      "cv_2045": 0.39157741175637767
    }
  },
  "warnings": [],
  "decisions": [
    {
      "decision_id": "D001",
      "timestamp": "2025-12-29T06:30:35.213700+00:00",
      "category": "data_integration",
      "decision": "Extracted estimates from 9 module categories",
      "rationale": "Consolidate all statistical findings for scenario modeling",
      "alternatives_considered": [
        "Use only time series models",
        "Use only cross-sectional results"
      ],
      "evidence": "Loaded 32 result files",
      "reversible": true
    },
    {
      "decision_id": "D002",
      "timestamp": "2025-12-29T06:30:35.213748+00:00",
      "category": "methodology",
      "decision": "Computed AIC and R-squared based model weights",
      "rationale": "AIC weights for time series averaging; R2 weights for cross-sectional insight",
      "alternatives_considered": [
        "Equal weights",
        "Expert-assigned weights",
        "BIC weights"
      ],
      "evidence": "AIC models: ['arima', 'var'], R2 models: ['ols', 'elastic_net', 'random_forest']",
      "reversible": true
    },
    {
      "decision_id": "D003",
      "timestamp": "2025-12-29T06:30:35.215670+00:00",
      "category": "scenario_design",
      "decision": "Generated 4 scenarios: CBO Full, Moderate, Zero, Pre-2020 Trend",
      "rationale": "Cover range from optimistic to restrictive immigration policy outcomes",
      "alternatives_considered": [
        "5-scenario approach",
        "Probabilistic weighting of scenarios"
      ],
      "evidence": "Baseline 2024 = 5126, Pre-2020 trend = 72.43/yr",
      "reversible": true
    },
    {
      "decision_id": "D004",
      "timestamp": "2025-12-29T06:30:35.245819+00:00",
      "category": "uncertainty_quantification",
      "decision": "Ran 1000-draw Monte Carlo simulation",
      "rationale": "Propagate parameter uncertainty through projections",
      "alternatives_considered": [
        "Analytical confidence intervals",
        "Bootstrapping"
      ],
      "evidence": "Trend uncertainty from quantile regression range",
      "reversible": true
    }
  ],
  "next_steps": [
    "Integrate scenario projections with cohort-component model",
    "Sensitivity analysis on key parameters",
    "Compare with official CBO and Census projections",
    "Update scenarios as new policy information becomes available",
    "Create interactive dashboard for scenario exploration"
  ]
}


================================================================================
TASK INSTRUCTIONS
================================================================================


## Your Task

### Part 1: Hazard Model Interpretation
1. What does the Cox proportional hazards model tell us about migration wave dynamics?
2. How should the baseline hazard be interpreted?
3. What do the covariate coefficients (if any) mean for forecasting?
4. Are the proportional hazards assumptions plausible for this application?

### Part 2: Theoretical Forecasting Bridge
Develop a theoretical framework connecting duration analysis to forecasting:

1. **Wave State Classification:**
   - How to determine where we are in the current wave?
   - Criteria for: early, peak, declining, ended
   - What data is needed for real-time classification?

2. **Expected Remaining Duration:**
   - Given we're at time t in a wave, what is E[remaining duration]?
   - Mathematical formula using the hazard estimates
   - Uncertainty quantification

3. **Transition Probability Forecasting:**
   - P(wave ends by period t+k | wave ongoing at t)
   - How does this feed into migration level forecasts?

4. **Integration with Time Series:**
   - How to combine hazard-based forecasts with ARIMA/scenario forecasts?
   - Should they be separate scenarios or formally combined?

### Part 3: Mathematical Specification
Provide mathematical specifications for:

1. **Survival function given current wave state:**
   S(t+k | T > t) = ?

2. **Expected migration path conditional on wave state:**
   E[y_{t+k} | wave state at t] = ?

3. **Uncertainty bands:**
   How to construct prediction intervals that account for both:
   - Uncertainty about wave transition timing
   - Uncertainty about migration levels within a wave

### Part 4: Operational Implementation
1. What data would be needed to implement this in practice?
2. How often should wave state be reassessed?
3. What are the limitations of this approach?
4. When would duration-based forecasting outperform simple time series methods?

### Part 5: Honest Limitations
1. With only a few observed waves, how reliable are the hazard estimates?
2. Is this genuinely useful for forecasting, or primarily descriptive?
3. How should this be framed in the paper to avoid overclaiming?

Please provide mathematically precise specifications that can be implemented.


--------------------------------------------------------------------------------
Total characters: 116075
Estimated tokens: 29018
